{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3CXAADdN6fv"
   },
   "source": [
    "###  Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BoaocMuoS-DF",
    "outputId": "fc9ffb7e-1ecc-4f8a-b3a0-af5e98b761ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Import Utilities\n",
    "from google.colab import drive\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download, notebook_login\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t3aC6pmP3BLT",
    "outputId": "3972b37b-0b99-4227-ee45-766b78e6d0c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/cultural-trends/scripts\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/cultural-trends/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HE4r4R0DnT6"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Av0IOTFDcpTx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TvJ1rmsx7FCy"
   },
   "outputs": [],
   "source": [
    "#%%writefile /content/drive/MyDrive/cultural-trends/scripts/utils.py\n",
    "import re\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "\n",
    "MAX_ATTEMPTS = 10\n",
    "\n",
    "def retry_request(url, payload, headers):\n",
    "    for i in range(MAX_ATTEMPTS):\n",
    "        try:\n",
    "            response = requests.post(url, data=json.dumps(\n",
    "                payload), headers=headers, timeout=90)\n",
    "            json_response = json.loads(response.content)\n",
    "            if \"error\" in json_response:\n",
    "                print(json_response)\n",
    "                print(f\"> Sleeping for {2 ** i}\")\n",
    "                time.sleep(2 ** i)\n",
    "            else:\n",
    "                return json_response\n",
    "        except:\n",
    "            print(f\"> Sleeping for {2 ** i}\")\n",
    "            time.sleep(2 ** i)  # exponential back off\n",
    "    raise TimeoutError()\n",
    "\n",
    "def convert_to_percentages(answers, options, answer_map=None, is_scale=False):\n",
    "    answers_mapped = []\n",
    "    for ans in answers:\n",
    "        if ans == -1: continue\n",
    "        if ans not in options and answer_map is not None:\n",
    "            answers_mapped += [str(answer_map[ans])]\n",
    "        elif not is_scale:\n",
    "            answers_mapped += [options[ans-1]]\n",
    "        else:\n",
    "            answers_mapped += [ans]\n",
    "\n",
    "    # Count the occurrences of each answer\n",
    "    answer_counts = Counter(answers_mapped)\n",
    "    # Calculate the total number of answers\n",
    "    total_answers = len(answers)\n",
    "    # Calculate the percentage for each unique answer and store it in a dictionary\n",
    "    percentages = {answer: (count / total_answers) * 100 for answer, count in answer_counts.items()}\n",
    "    labels = list(percentages.keys())\n",
    "    values = [percentages[label] if label in percentages else 0 for label in labels]\n",
    "    return options, values\n",
    "\n",
    "def parse_range(data):\n",
    "    \"\"\"\n",
    "    Turns a dictionary with number ranges as keys into one with single numbers as keys.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary with keys as strings like \"1-3\" or \"5\", and any values.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary with numbers as keys and the same values copied over.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    for q_range in data:\n",
    "        if \"-\" in q_range:\n",
    "            q_start, q_end = tuple(map(int, q_range.split(\"-\")))\n",
    "        else:\n",
    "            q_start = q_end = int(q_range)\n",
    "\n",
    "        for q_idx in range(q_start, q_end+1):\n",
    "            data_dict[q_idx] = data[q_range]\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def cartesian_product(lists):\n",
    "    return list(product(*lists))\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, 'r', encoding=\"utf-8\") as fin:   # Open file for reading\n",
    "        data = fin.readlines()                       # Read all lines into a list\n",
    "    return data\n",
    "\n",
    "def read_raw(path):\n",
    "    with open(path, 'r', encoding=\"utf-8\") as fin:\n",
    "        data = fin.read()\n",
    "    return data\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, 'r', encoding=\"utf-8\") as fin:\n",
    "        data = json.load(fin)\n",
    "    return data\n",
    "\n",
    "def read_yaml(path):\n",
    "    with open(path, 'r', encoding=\"utf-8\") as fin:\n",
    "        data = yaml.load(fin, Loader=yaml.FullLoader)\n",
    "    return data\n",
    "\n",
    "def write_file(path, data):\n",
    "    with open(path, 'w', encoding=\"utf-8\") as fout:\n",
    "        fout.write('\\n'.join(data))\n",
    "\n",
    "def write_json(path, data):\n",
    "    with open(path, 'w', encoding=\"utf-8\") as fout:\n",
    "        json.dump(data, fout, ensure_ascii=False)\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "\n",
    "def jensen_shannon_distance(p, q):\n",
    "    \"\"\"\n",
    "    method to compute the Jenson-Shannon Distance\n",
    "    between two probability distributions\n",
    "    \"\"\"\n",
    "\n",
    "    # convert the vectors into numpy arrays in case that they aren't\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "\n",
    "    # calculate m\n",
    "    m = (p + q) / 2\n",
    "\n",
    "    # compute Jensen Shannon Divergence\n",
    "    divergence = (scipy.stats.entropy(p, m) + scipy.stats.entropy(q, m)) / 2\n",
    "\n",
    "    # compute the Jensen Shannon Distance\n",
    "    distance = np.sqrt(divergence)\n",
    "\n",
    "    return distance\n",
    "\n",
    "def append_row(\n",
    "    data,\n",
    "    **cols,\n",
    "):\n",
    "    for k, v in cols.items():\n",
    "        data[k].append(v)\n",
    "\n",
    "\n",
    "def parse_response_wvs(response: str, question_options: list):\n",
    "  \"\"\"\n",
    "    Parse a raw model-generated response into a 1-based index for a World Values Survey question.\n",
    "\n",
    "    Parameters:\n",
    "        response (str): The raw text produced by the model.\n",
    "        question_options (list[str]): The list of valid answer option strings, in order.\n",
    "\n",
    "    Returns:\n",
    "        int: A 1-based index corresponding to one of the `question_options`, or -1 if parsing fails.\n",
    "  \"\"\"\n",
    "\n",
    "  # Make response case-insensitive\n",
    "  response = response.lower().strip()\n",
    "\n",
    "  # Matches a number enclosed in parentheses\n",
    "  pattern = r\"\\(\\d+\\)\"\n",
    "\n",
    "  # Search for the number within response\n",
    "  match = re.search(pattern, response)\n",
    "\n",
    "  if match:\n",
    "\n",
    "      # Extract number from match and convert it to integer\n",
    "      match_text = match.group()\n",
    "      stripped_text = match_text[1:-1]\n",
    "      answer = int(stripped_text)\n",
    "\n",
    "      return answer if 1 <= answer <= len(question_options) else -1\n",
    "\n",
    "  else:\n",
    "      # for option_idx, option in enumerate(question_options):\n",
    "      #     if response == option.lower().strip():\n",
    "      #         return option_idx+1\n",
    "      # question_options = question_options[::-1]\n",
    "      # for option_idx, option in enumerate(question_options):\n",
    "      #     if response == option.lower().strip():\n",
    "      #         return len(question_options)-option_idx\n",
    "\n",
    "\n",
    "      # Match model response to one of the  valid options\n",
    "      for option_idx, option in enumerate(question_options):\n",
    "          if response in option.lower().strip():\n",
    "              return option_idx+1\n",
    "\n",
    "  # Handle cases where closing parenthesis is missing\n",
    "  pattern = r\"\\(\\d+\"\n",
    "  match = re.search(pattern, response)\n",
    "  if match:\n",
    "      answer = int(match.group()[1:])\n",
    "      return answer if 1 <= answer <= len(question_options) else -1\n",
    "\n",
    "  # Takes the first number of the response\n",
    "  pattern = r\"\\d+\"\n",
    "  match = re.search(pattern, response)\n",
    "  if match:\n",
    "      answer = int(match.group())\n",
    "      return answer if 1 <= answer <= len(question_options) else -1\n",
    "\n",
    "  #  Unable to parse a valid index\n",
    "  return -1\n",
    "\n",
    "def parse_response(res: str, options: list):\n",
    "    if type(res) == int:\n",
    "        return res\n",
    "\n",
    "    res = res.strip()\n",
    "    pattern = r\"\\d+\"\n",
    "    match = re.search(pattern, res)\n",
    "    if match:\n",
    "        answer = int(match.group())\n",
    "        if 1 <= answer <= len(options):\n",
    "            return answer\n",
    "\n",
    "    num_words = len(res.split())\n",
    "    for i, option in enumerate(options):\n",
    "        space_idx = option.index(\" \")\n",
    "        if res == option or \\\n",
    "           res == option.replace(\".\", \"\").strip() or \\\n",
    "           res == option[space_idx+1:].strip() or \\\n",
    "           res == option[space_idx+1:].strip().replace(\".\", \"\") or \\\n",
    "           res == ' '.join(option[:num_words]):\n",
    "            return i+1\n",
    "\n",
    "    for i in range(1, len(options)+1):\n",
    "        if str(i) in res:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def parse_question(q: dict, questions_en=None):\n",
    "    index = '.'.join(str(x) for x in q['index'])\n",
    "    text = q['questions'][0]\n",
    "\n",
    "    if questions_en is not None:\n",
    "        options = questions_en[index][\"options\"]\n",
    "    else:\n",
    "        options = q[\"options\"]\n",
    "\n",
    "    qparams = q[\"question_parameters\"] if \"question_parameters\" in q else None\n",
    "\n",
    "    return {\n",
    "        'index': index,\n",
    "        'text': text,\n",
    "        'options': options,\n",
    "        \"qparams\": qparams\n",
    "    }\n",
    "\n",
    "def append_data(qidx, data, questions, columnar_data):\n",
    "\n",
    "    invalid_ans = 0\n",
    "    for row in data:\n",
    "        try:\n",
    "            if \"Error\" in row:\n",
    "                continue\n",
    "            persona = row['persona']\n",
    "            qid = '.'.join(str(x) for x in row['question']['id'])\n",
    "            vid = row['question']['variant']\n",
    "            responses = row['response']\n",
    "            qparams = row[\"question\"][\"params\"]\n",
    "            key_qparam = list(qparams.keys())[0] if len(qparams) > 0 else None\n",
    "            if qidx == 6 and qparams[key_qparam] in [\"Corporations\", \"Public Companies\",\"Local Government\", \"Electoral Process\"]:\n",
    "                continue\n",
    "\n",
    "            for response in responses:\n",
    "\n",
    "                question = questions[qid]\n",
    "                options = question[\"options\"]\n",
    "                answer = parse_response(response, options)\n",
    "                if answer == -1:\n",
    "                    invalid_ans += 1\n",
    "                    continue\n",
    "\n",
    "                if key_qparam is not None:\n",
    "                    qparam_idx = str(question[\"qparams\"][key_qparam].index(qparams[key_qparam]) + 1)\n",
    "                else:\n",
    "                    qparam_idx = \"0\"\n",
    "\n",
    "                if qidx == 10 and qparam_idx == \"2\":\n",
    "                    # to remove the extra variant Nael added\n",
    "                    continue\n",
    "\n",
    "                # breakpoint()\n",
    "\n",
    "                append_row(\n",
    "                    columnar_data,\n",
    "                    qid=qid, vid=vid, response=answer,\n",
    "                    question_text=question['text'],\n",
    "                    response_text=question['options'][answer-1],\n",
    "                    qparam_id=qparam_idx,\n",
    "                    **persona,\n",
    "                )\n",
    "        except:\n",
    "            breakpoint()\n",
    "            raise\n",
    "\n",
    "    print('='*50)\n",
    "    print(f\"> {invalid_ans} Invalid Answers\")\n",
    "    print('='*50)\n",
    "    return columnar_data, invalid_ans\n",
    "\n",
    "def read_question(path, qidx, questions_en=None):\n",
    "    questions = {}\n",
    "    with open(path, 'r', encoding='utf-8') as fp:\n",
    "        q_data = yaml.safe_load(fp)['dataset']\n",
    "        for row in q_data:\n",
    "            if row[\"index\"][0] != qidx: continue\n",
    "            q = parse_question(row, questions_en)\n",
    "            questions[q['index']] = q\n",
    "    return questions\n",
    "\n",
    "def get_results_path(filesuffix, model_name, lang, version, m1):\n",
    "    for v_num in range(version, 0, -1):\n",
    "        if m1:\n",
    "            v_num = f\"{v_num}m1\"\n",
    "        results_path = f'results/{model_name}/{lang}/preds_{filesuffix}_v{v_num}.json'\n",
    "        if os.path.exists(results_path):\n",
    "            return results_path\n",
    "    return None\n",
    "\n",
    "def append_response(\n",
    "    model_data:list[dict],\n",
    "    row:dict,\n",
    "    response_int:int,\n",
    "    response_id:int,\n",
    "    persona_id: int,\n",
    "    q_responses:list[int]\n",
    "    ):\n",
    "\n",
    "  \"\"\"\n",
    "  Append a single response record to the flattened model dataset,\n",
    "  optionally performing majority-voting over multiple parsed answers.\n",
    "\n",
    "  Parameters:\n",
    "    model_data (list[dict]): A new record is appended.\n",
    "    row (dict): Raw entry containing keys:\n",
    "            - \"persona\": sub-dict with demographic fields.\n",
    "            - \"question\": sub-dict with \"id\" and \"variant\" (0-based) fields.\n",
    "    response_int (int):\n",
    "        The initially parsed integer response (1-based).\n",
    "    response_id (int):\n",
    "        Index (0-based) of this particular generation attempt within the variant.\n",
    "    persona_id (int):\n",
    "        Sequential index of the persona/variant block in the original dataset (used\n",
    "        for tracking but not stored in the appended record).\n",
    "    q_responses (list[int] | None):\n",
    "        If not None, a list of multiple parsed integer responses collected for\n",
    "        majority-voting.\n",
    "\n",
    "  Returns:\n",
    "      list[dict]:\n",
    "          The same `model_data` list with one new appended response record.\n",
    "\n",
    "  Raises:\n",
    "      AssertionError:/\n",
    "          If the final `response_int` (after voting) is not greater than zero.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # If q_response was provided, ignore response_int and do majority voting\n",
    "  if q_responses is not None:\n",
    "      # Count each unique response\n",
    "      response_counter = Counter(q_responses)\n",
    "\n",
    "      # Get them sorted by frecuency\n",
    "      most_common_responses = response_counter.most_common()\n",
    "\n",
    "      # First is the highest frecuency as it is sorted\n",
    "      first_freq = most_common_responses[0]\n",
    "\n",
    "      # Extract its frecuency\n",
    "      max_freq = first_freq[1]\n",
    "\n",
    "      # Collect tied winners\n",
    "      max_responses = []\n",
    "\n",
    "      # Iterate list frecuency till counts drop below max_freq\n",
    "      for most_common in most_common_responses:\n",
    "          if most_common[1] == max_freq:\n",
    "              # Collect all tied for first\n",
    "              max_responses += [most_common[0]]\n",
    "          else:\n",
    "              break\n",
    "      # If multiple equally common answers, pick one at random\n",
    "      response_int = np.random.choice(max_responses)\n",
    "\n",
    "  assert response_int > 0\n",
    "\n",
    "  # Append structured response to model data\n",
    "\n",
    "  model_data += [{\n",
    "      \"persona.region\": row[\"persona\"][\"region\"],\n",
    "      \"persona.sex\": row[\"persona\"][\"sex\"],\n",
    "      \"persona.age\": row[\"persona\"][\"age\"],\n",
    "      \"persona.country\": row[\"persona\"][\"country\"],\n",
    "      \"persona.marital_status\": row[\"persona\"][\"marital_status\"],\n",
    "      \"persona.education\": row[\"persona\"][\"education\"],\n",
    "      \"persona.social_class\": row[\"persona\"][\"social_class\"],\n",
    "      \"question.id\": row[\"question\"][\"id\"],\n",
    "      \"question.variant\": row[\"question\"][\"variant\"],\n",
    "      \"response.id\": response_id, # Which generation\n",
    "      \"response.answer\": response_int\n",
    "  }]\n",
    "  return model_data\n",
    "\n",
    "\n",
    "def convert_to_dataframe(\n",
    "    model_data:list[dict],\n",
    "    question_options:list[str],\n",
    "    demographic_map: dict[str,str],\n",
    "    eval_method: str = \"mv_all\",\n",
    "    language: str = \"en\",\n",
    "    is_scale_question: bool = False\n",
    "    ):\n",
    "\n",
    "  \"\"\" Flatten and evaluate raw model outputs into a structured DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        model_data (list[dict]):\n",
    "            Raw output for a single question across all personas and variants.\n",
    "\n",
    "        question_options (list[str]):\n",
    "            The ordered list of valid option strings for this question, used\n",
    "            to validate and parse model outputs.\n",
    "\n",
    "        eval_method (str, default=\"mv_all\")\n",
    "\n",
    "        language (str, default=\"en\")\n",
    "\n",
    "        is_scale_question (bool, default=False):\n",
    "            If True, collapse 10-point scale answers into 5 bins via ceil(ans/2).\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, int]:\n",
    "            - DataFrame with columns:\n",
    "                persona.region, persona.sex, persona.age, persona.country,\n",
    "                persona.marital_status, persona.education, persona.social_class,\n",
    "                question.id, question.variant,\n",
    "                response.id, response.answer\n",
    "              One row per persona-question according to `eval_method`.\n",
    "            - invalid_count (int): number of generations that could not be\n",
    "              parsed into a valid answer.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError:\n",
    "            If `eval_method` is invalid or if data ordering is wrong\n",
    "            (expects variants 0–3 in sequence), or if a voted response\n",
    "            is not a positive integer.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  assert eval_method in {\"flatten\", \"mv_sample\", \"mv_all\", \"first\"}\n",
    "\n",
    "  model_data_flat = []\n",
    "  invalid_count = 0\n",
    "  # Collect parsed ints for voting\n",
    "  q_responses = []\n",
    "\n",
    "  # Loop every raw row of model output\n",
    "  for row_idx, row in enumerate(model_data):\n",
    "\n",
    "      #if language != \"en\":\n",
    "      #    row[\"persona\"] = {d_text: (demographic_map[d_text][d_value] if d_text != \"region\" else demographic_map[d_text][d_value]) if d_text != \"age\" else d_value for d_text, d_value in row[\"persona\"].items()}\n",
    "\n",
    "      # Clear out any old responses at the start of each row.\n",
    "      if eval_method == \"mv_sample\":\n",
    "          q_responses = []\n",
    "\n",
    "      # Reset every 4 rows (one set of variants)\n",
    "      if row_idx % 4 == 0 and eval_method == \"mv_all\":\n",
    "          q_responses = []\n",
    "\n",
    "      # Verify data is in groups of 4 (1 persona - 4 variants)\n",
    "      # assert row_idx % 4 == row[\"question\"][\"variant\"]\n",
    "\n",
    "      # Parse each generation (5 of them per row)\n",
    "      for response_id, response in enumerate(row[\"response\"]):\n",
    "\n",
    "          # Turns the row string into 1..N or -1\n",
    "          response_int = parse_response_wvs(response, question_options)\n",
    "\n",
    "          if is_scale_question:\n",
    "              response_int = int(np.ceil(response_int/2))\n",
    "\n",
    "          if eval_method == \"first\":\n",
    "\n",
    "              # If first parse response is invalid, just count and stop\n",
    "              if response_int <= 0:\n",
    "                  invalid_count += 1\n",
    "              else:\n",
    "              # Append the one and break (ignoring the other 4 generations)\n",
    "                  model_data_flat = append_response(model_data_flat, row, response_int, response_id, row_idx, q_responses=None)\n",
    "              break\n",
    "\n",
    "          if eval_method == \"flatten\":\n",
    "              # Every valid generation becomes a row\n",
    "              if response_int <= 0:\n",
    "                  invalid_count += 1\n",
    "                  continue\n",
    "              model_data_flat = append_response(model_data_flat, row, response_int, response_id, row_idx, q_responses=None)\n",
    "\n",
    "          elif response_int > 0 and \"mv\" in eval_method:\n",
    "              # Collect valid responses for later voting\n",
    "              q_responses += [response_int]\n",
    "\n",
    "      if eval_method == \"mv_sample\":\n",
    "          # if no valid responses, count it\n",
    "          if len(q_responses) == 0:\n",
    "              # breakpoint()\n",
    "              invalid_count += 1\n",
    "              continue\n",
    "\n",
    "          # Pass q_response list for voting\n",
    "          model_data_flat = append_response(model_data_flat, row, -1, response_id, row_idx, q_responses)\n",
    "\n",
    "      elif eval_method == \"mv_all\" and row_idx % 4 == 3: # vote in 4 variant\n",
    "\n",
    "          if len(q_responses) == 0:\n",
    "              invalid_count += 1\n",
    "              continue\n",
    "          # Vote over all 20 responses\n",
    "          model_data_flat = append_response(model_data_flat, row, -1, response_id, row_idx, q_responses)\n",
    "\n",
    "  return pd.DataFrame(model_data_flat), invalid_count\n",
    "\n",
    "def create_wvs_question_map(headers:list[str], selected_questions:list[str]):\n",
    "  \"\"\"\n",
    "    Creates a mapping from WVS question indices to their corresponding column names\n",
    "    in the dataset, filtered by a list of selected question indices.\n",
    "\n",
    "    Args:\n",
    "        headers (List[str]): List of column names from the WVS dataset.\n",
    "        selected_questions (List[int]): List of numeric question indices to retain.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, str]: A dictionary mapping question indices to column names.\n",
    "    \"\"\"\n",
    "  wvs_question_map = {}\n",
    "  for column in headers:\n",
    "    match = re.search(r\"Q(\\d+)[\\w]? (.+)\", column) #A number following \"Q\" (Q(\\d+)) / word character after the number ([\\w]?)\n",
    "    if match:\n",
    "      qidx = int(match.group(1))\n",
    "      if qidx in selected_questions:\n",
    "        wvs_question_map[qidx] = column\n",
    "  return wvs_question_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYwxWvaVCFmY"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udxG2BkR1A0U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-7qtgoc5bQL",
    "outputId": "3ffdecbd-bafe-4e29-b8fb-7aa5d20c14d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 Personas\n",
      "31\n",
      "Imagine you are a married male from Illinois, United States. \n",
      "You are 46 years of age and completed higher education level.\n",
      "You consider yourself part of the lower middle class.\n",
      "Answer strictly from this persona’s point of view.\n",
      "Select exactly one option. Do not include any commentary.\n",
      "Return only the number of the chosen option. No words or punctuation.\n",
      "Question: How important are friends in life?\n",
      "Options:\n",
      "(1) Very important\n",
      "(2) Rather important\n",
      "(3) Not very important\n",
      "(4) Not at all important\n",
      "(5) Don't know\n"
     ]
    }
   ],
   "source": [
    "#%%writefile /content/drive/MyDrive/cultural-trends/scripts/wvs_dataset.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import read_json, read_yaml, read_file, read_raw, parse_range, create_wvs_question_map\n",
    "\n",
    "scale_option_template = {\n",
    "    \"en\": \"To indicate your opinion, use a 10-point scale where “1” means “{}” and “10” means “{}”.\",\n",
    "    \"ar\": \"للتعبير عن رأيك، استخدم مقياسًا من 10 نقاط حيث تشير ”1“ إلى {} وتشير ”10“ إلى {}.\"\n",
    "}\n",
    "\n",
    "jais_prompt_en = \"\"\"### Instruction: Your name is Jais, and you are named after Jebel Jais, the highest mountain in UAE.\n",
    " You are built by Inception and MBZUAI. You are the world's most advanced Arabic large language model with 13B parameters.\n",
    " You outperform all existing Arabic models by a sizable margin and you are very competitive with English models of similar size.\n",
    " You can answer in Arabic and English only.\n",
    " You are a helpful, respectful and honest assistant.\n",
    " When answering, abide by the following guidelines meticulously: Always answer as helpfully as possible, while being safe.\n",
    " Your answers should not include any harmful, unethical, racist, sexist, explicit, offensive, toxic, dangerous, or illegal content.\n",
    "\n",
    " Do not give medical, legal, financial, or professional advice. Never assist in or promote illegal activities.\n",
    " Always encourage legal and responsible actions. Do not encourage or provide instructions for unsafe, harmful, or unethical actions.\n",
    " Do not create or share misinformation or fake news. Please ensure that your responses are socially unbiased and positive in nature.\n",
    " If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    " If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    " Prioritize the well-being and the moral integrity of users.\n",
    " Avoid using toxic, derogatory, or offensive language.\n",
    " Maintain a respectful tone. Do not generate, promote, or engage in discussions about adult content.\n",
    " Avoid making comments, remarks, or generalizations based on stereotypes.\n",
    " Do not attempt to access, produce, or spread personal or private information. Always respect user confidentiality.\n",
    " Stay positive and do not say bad things about anything.\n",
    " Your primary objective is to avoid harmful responses, even when faced with deceptive inputs.\n",
    " Recognize when users may be attempting to trick or to misuse you and respond with caution.\n",
    " \\n\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input: [|Human|] {Question}\\n### Response: [|AI|]\"\n",
    "jais_prompt_ar = \"### Instruction: اسمك جيس وسميت على اسم جبل جيس اعلى جبل في الامارات. تم بنائك بواسطة Inception و MBZUAI. أنت نموذج اللغة العربية الأكثر تقدمًا في العالم مع بارامترات 13B. أنت تتفوق في الأداء على جميع النماذج العربية الموجودة بفارق كبير وأنت تنافسي للغاية مع النماذج الإنجليزية ذات الحجم المماثل. يمكنك الإجابة باللغتين العربية والإنجليزية فقط. أنت مساعد مفيد ومحترم وصادق. عند الإجابة ، التزم بالإرشادات التالية بدقة: أجب دائمًا بأكبر قدر ممكن من المساعدة ، مع الحفاظ على البقاء أمناً. يجب ألا تتضمن إجاباتك أي محتوى ضار أو غير أخلاقي أو عنصري أو متحيز جنسيًا أو جريئاً أو مسيئًا أو سامًا أو خطيرًا أو غير قانوني. لا تقدم نصائح طبية أو قانونية أو مالية أو مهنية. لا تساعد أبدًا في أنشطة غير قانونية أو تروج لها. دائما تشجيع الإجراءات القانونية والمسؤولة. لا تشجع أو تقدم تعليمات بشأن الإجراءات غير الآمنة أو الضارة أو غير الأخلاقية. لا تنشئ أو تشارك معلومات مضللة أو أخبار كاذبة. يرجى التأكد من أن ردودك غير متحيزة اجتماعيًا وإيجابية بطبيعتها. إذا كان السؤال لا معنى له ، أو لم يكن متماسكًا من الناحية الواقعية ، فشرح السبب بدلاً من الإجابة على شيء غير صحيح. إذا كنت لا تعرف إجابة السؤال ، فالرجاء عدم مشاركة معلومات خاطئة. إعطاء الأولوية للرفاهية والنزاهة الأخلاقية للمستخدمين. تجنب استخدام لغة سامة أو مهينة أو مسيئة. حافظ على نبرة محترمة. لا تنشئ أو تروج أو تشارك في مناقشات حول محتوى للبالغين. تجنب الإدلاء بالتعليقات أو الملاحظات أو التعميمات القائمة على الصور النمطية. لا تحاول الوصول إلى معلومات شخصية أو خاصة أو إنتاجها أو نشرها. احترم دائما سرية المستخدم. كن إيجابيا ولا تقل أشياء سيئة عن أي شيء. هدفك الأساسي هو تجنب الاجابات المؤذية ، حتى عند مواجهة مدخلات خادعة. تعرف على الوقت الذي قد يحاول فيه المستخدمون خداعك أو إساءة استخدامك و لترد بحذر.\\n\\nأكمل المحادثة أدناه بين [|Human|] و [|AI|]:\\n### Input: [|Human|] {Question}\\n### Response: [|AI|]\"\n",
    "\n",
    "\"\"\"\n",
    "class WVSDataset:\n",
    "    def __init__(self, filepath,\n",
    "            language=\"en\",\n",
    "            country=\"us\",\n",
    "            fewshot=0,\n",
    "            api=False,\n",
    "            model_name=None,\n",
    "            use_anthro_prompt=False,\n",
    "        ):\n",
    "\n",
    "        self.dataset = {}\n",
    "        self.persona_qid = {}\n",
    "        self.question_info = {}\n",
    "        self.responses = {}\n",
    "\n",
    "        self.fewshot_dataset = {}\n",
    "        self.fewshot_persona_qid = {}\n",
    "        self.fewshot_question_info = {}\n",
    "        self.fewshot_responses = {}\n",
    "\n",
    "        self.persona = []\n",
    "        self.raw_responses = []\n",
    "        self.is_api = api\n",
    "        self.language = language\n",
    "        self.country = country\n",
    "        self.is_jais = model_name==\"jais-13b-chat\" if model_name is not None else False\n",
    "        self.fewshot = fewshot\n",
    "        self.model_name = model_name\n",
    "        self.use_anthro_prompt = use_anthro_prompt\n",
    "\n",
    "        filter_questions = [qid.strip() for qid in read_raw(\"../dataset/selected_questions.csv\").split(\",\")]\n",
    "\n",
    "        wvs_questions_path = f\"../dataset/wvs_questions_dump.{language}.json\"\n",
    "        self.wvs_questions = {q_id: q_val for q_id, q_val in read_json(wvs_questions_path).items() if q_id in filter_questions}\n",
    "\n",
    "        self.anthro_templ = read_yaml(\"../dataset/wvs_template_anthro_framework.yml\")[\"template_values\"]\n",
    "\n",
    "        template_data = read_yaml(filepath)\n",
    "\n",
    "        self.create_dataset(template_data)\n",
    "        self.set_question(index=2)\n",
    "\n",
    "    def set_question(self, index):\n",
    "        self.current_question_index = index\n",
    "\n",
    "    def trim_dataset(self, start_index):\n",
    "        qidx = f\"Q{self.current_question_index}\"\n",
    "        self.dataset[qidx] = self.dataset[qidx][start_index:]\n",
    "\n",
    "        self.persona_qid[qidx] = self.persona_qid[qidx][start_index:]\n",
    "        self.question_info[qidx] = self.question_info[qidx][start_index:]\n",
    "\n",
    "    @property\n",
    "    def question_ids(self):\n",
    "        return list(self.wvs_questions.keys())\n",
    "\n",
    "    def create_dataset(self, template_data):\n",
    "        if self.country == \"egypt\":\n",
    "            path = \"../dataset/eg_wvs_wave7_v7_n303.csv\"\n",
    "        elif self.country == \"us\":\n",
    "            #path = \"../dataset/F00013339-WVS_Wave_7_United_States_CsvTxt_v5.0.csv\"\n",
    "            path = \"../dataset/us_wvs_wave7_v7_n303.csv\"\n",
    "\n",
    "        survey_df = pd.read_csv(path, header=0, delimiter=\";\")\n",
    "\n",
    "        demographic_ids = [\"N_REGION_WVS Region country specific\", \"Q260 Sex\", \"Q262 Age\", \"Q266 Country of birth: Respondent\",\n",
    "                           \"Q273 Marital status\", \"Q275R Highest educational level: Respondent (recoded into 3 groups)\", \"Q287 Social class (subjective)\"]\n",
    "        demographic_txt = [\"region\", \"sex\", \"age\", \"country\",\n",
    "                           \"marital_status\", \"education\", \"social_class\"]\n",
    "\n",
    "        print(f\"{len(survey_df)} Personas\")\n",
    "\n",
    "        template_0 = template_data[\"template\"][0]\n",
    "        template_1 = template_data[\"template\"][1]\n",
    "\n",
    "        template_parameters = template_data[\"template_values\"]\n",
    "\n",
    "        if self.language == \"en\" and self.use_anthro_prompt:\n",
    "            prompt_template = self.anthro_templ[\"prompt\"]\n",
    "        # elif self.language == \"ar\" and self.model_name == \"Llama-2-13b-chat-hf\":\n",
    "        #     prompt_template = template_parameters[\"prompt_variants\"][2]\n",
    "        elif self.language == \"ar\" and self.model_name == \"AceGPT-13B-chat\":\n",
    "            prompt_template = template_parameters[\"prompt_variants\"][2]\n",
    "        elif self.country == \"us\" and self.language == \"ar\":\n",
    "            prompt_template = template_parameters[\"prompt_variants\"][1]\n",
    "        else:\n",
    "            prompt_template = template_parameters[\"prompt_variants\"][0]\n",
    "\n",
    "        question_header = template_parameters[\"question_header\"]\n",
    "        options_header = template_parameters[\"options_header\"]\n",
    "\n",
    "        ar_persona_parameters = template_data[\"persona_parameters\"]\n",
    "\n",
    "        country_cap = \"US\" if self.country == \"us\" else \"Egypt\"\n",
    "\n",
    "        selected_questions = read_file(\"../dataset/selected_questions.csv\")[0].split(\",\")\n",
    "        selected_questions = list(map(str.strip, selected_questions))\n",
    "        selected_questions = [int(qnum[1:]) for qnum in selected_questions]\n",
    "\n",
    "        wvs_question_map = create_wvs_question_map(survey_df.columns.tolist(), selected_questions)\n",
    "\n",
    "        wvs_response_map = read_json(\"../dataset/wvs_response_map.json\")\n",
    "\n",
    "        options_dict = parse_range(read_json(\"../dataset/wvs_options.json\"))\n",
    "\n",
    "        if self.language != \"en\":\n",
    "            demographic_map = {}\n",
    "            en_template_data = read_yaml(f\"../dataset/wvs_template.en.yml\")\n",
    "            for d_text in demographic_txt:\n",
    "                if d_text == \"age\": continue\n",
    "                d_text_cap = ' '.join(list(map(str.capitalize, d_text.replace(\"_\", \" \").split())))\n",
    "                if d_text == \"region\":\n",
    "                    d_values = en_template_data[\"persona_parameters\"][d_text_cap][country_cap]\n",
    "                else:\n",
    "                    d_values = en_template_data[\"persona_parameters\"][d_text_cap]\n",
    "\n",
    "                demographic_map[d_text] = {}\n",
    "                for d_val_idx, d_val in enumerate(d_values):\n",
    "                    if d_text == \"region\":\n",
    "                        demographic_map[d_text][d_val] = ar_persona_parameters[d_text_cap][country_cap][d_val_idx]\n",
    "                    else:\n",
    "                        demographic_map[d_text][d_val] = ar_persona_parameters[d_text_cap][d_val_idx]\n",
    "\n",
    "        if self.language == \"en\":\n",
    "            for _, row in survey_df.iterrows():\n",
    "                if self.country == \"us\" and row[\"Q266 Country of birth: Respondent\"] != \"United States\":\n",
    "                    continue\n",
    "                prompt_values = {demographic_key: row[demographic_id]\n",
    "                    if demographic_key in [\"age\", \"region\", \"country\"]\n",
    "                    else row[demographic_id].lower()\n",
    "                    for demographic_key, demographic_id in zip(demographic_txt, demographic_ids)\n",
    "                }\n",
    "\n",
    "                self.raw_responses += [{qidx: row[qkey] for qidx, qkey in wvs_question_map.items()}]\n",
    "                self.persona += [prompt_values]\n",
    "        else:\n",
    "            start_region_idx = 3 if self.country == \"us\" else 0\n",
    "            for _, row in survey_df.iterrows():\n",
    "                if self.country == \"us\" and row[\"Q266 Country of birth: Respondent\"] != \"United States\":\n",
    "                    continue\n",
    "                prompt_values = {demographic_key: demographic_map[demographic_key][row[demographic_id].split(\":\")[-1][start_region_idx:].strip() if demographic_key == \"region\" else row[demographic_id]]\n",
    "                    if demographic_key != \"age\"\n",
    "                    else row[demographic_id]\n",
    "                    for demographic_key, demographic_id in zip(demographic_txt, demographic_ids)\n",
    "                }\n",
    "                self.raw_responses += [{qidx: row[qkey] for qidx, qkey in wvs_question_map.items()}]\n",
    "                self.persona += [prompt_values]\n",
    "\n",
    "        if self.language == \"en\":\n",
    "            for prompt_values in self.persona:\n",
    "                prompt_values[\"region\"] = prompt_values[\"region\"].split(\":\")[-1].strip()\n",
    "                if self.country == \"us\":\n",
    "                    prompt_values[\"region\"] = prompt_values[\"region\"][2:].strip()\n",
    "\n",
    "        for qid, qdata in self.wvs_questions.items():\n",
    "            self.dataset[qid] = []\n",
    "            self.persona_qid[qid] = []\n",
    "            self.question_info[qid] = []\n",
    "            self.responses[qid] = []\n",
    "\n",
    "            self.fewshot_dataset[qid] = []\n",
    "            self.fewshot_persona_qid[qid] = []\n",
    "            self.fewshot_question_info[qid] = []\n",
    "            self.fewshot_responses[qid] = []\n",
    "\n",
    "            question_options = qdata[\"options\"]\n",
    "            for persona_idx, prompt_values in enumerate(self.persona):\n",
    "                for variant_idx, question in enumerate(qdata[\"questions\"]):\n",
    "                    if variant_idx > 0: continue\n",
    "                    prompt = prompt_template.format(**prompt_values)\n",
    "\n",
    "                    if \"chat\" in self.model_name:\n",
    "                        prompt = \"[INST] <<SYS>>\\n\" + prompt + \"\\n<</SYS>>\\n\"\n",
    "\n",
    "                    if \"scale\" in qdata and qdata[\"scale\"] == True:\n",
    "                        final_question = template_1.format(**{\n",
    "                            \"prompt\": prompt,\n",
    "                            \"question_header\": question_header,\n",
    "                            \"question\": question,\n",
    "                            \"scale\": scale_option_template[self.language].format(question_options[0], question_options[1]),\n",
    "                        })\n",
    "                    else:\n",
    "\n",
    "                        final_question = template_0.format(**{\n",
    "                            \"prompt\": prompt,\n",
    "                            \"question\": question,\n",
    "                            \"options\": '\\n'.join(f\"({option_idx+1}) {option}\" for option_idx, option in enumerate(question_options)),\n",
    "                            \"options_header\": options_header,\n",
    "                            \"question_header\": question_header,\n",
    "                        })\n",
    "\n",
    "                    if self.use_anthro_prompt:\n",
    "                        final_question = self.anthro_templ[\"anthro_prompt\"] + '\\n\\n' + final_question\n",
    "\n",
    "                    if \"chat\" in self.model_name:\n",
    "                        final_question += \" [/INST]\"\n",
    "\n",
    "                    qid_int = int(qid[1:])\n",
    "                    response = self.raw_responses[persona_idx][qid_int]\n",
    "                    response_map = {key: int(val) for key, val in wvs_response_map[str(qid_int)].items()}\n",
    "                    response_map |= {key: val+1 for val, key in enumerate(options_dict[qid_int])}\n",
    "                    response_map[\"No answer\"] = -1\n",
    "\n",
    "                    if persona_idx >= len(self.persona)-self.fewshot:\n",
    "                        self.fewshot_responses[qid] += [response_map[response]]\n",
    "                        self.fewshot_dataset[qid] += [final_question]\n",
    "                        self.fewshot_persona_qid[qid] += [prompt_values]\n",
    "                        self.fewshot_question_info[qid] += [{\n",
    "                            \"id\": qid,\n",
    "                            \"variant\": variant_idx,\n",
    "                        }]\n",
    "                    else:\n",
    "                        self.responses[qid] += [response_map[response]]\n",
    "                        self.dataset[qid] += [final_question]\n",
    "                        self.persona_qid[qid] += [prompt_values]\n",
    "                        self.question_info[qid] += [{\n",
    "                            \"id\": qid,\n",
    "                            \"variant\": variant_idx,\n",
    "                        }]\n",
    "\n",
    "    def fewshot_examples(self):\n",
    "        qidx = f\"Q{self.current_question_index}\"\n",
    "        num_question_variants = 4\n",
    "\n",
    "        variant_indices = np.random.choice(np.arange(num_question_variants), size=self.fewshot)\n",
    "\n",
    "        fewshots = []\n",
    "        responses = []\n",
    "        for idx in range(self.fewshot):\n",
    "            # fewshot_question_idx = index % num_question_variants + num_question_variants * idx\n",
    "            fewshot_question_idx = variant_indices[idx] + num_question_variants * idx\n",
    "            response = self.fewshot_responses[qidx][fewshot_question_idx]\n",
    "            fewshots += [self.fewshot_dataset[qidx][fewshot_question_idx] + f'\\nAnswer: {response}']\n",
    "            responses += [response]\n",
    "\n",
    "        return '\\n\\n'.join(fewshots) + '\\n\\n', responses\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        qidx = f\"Q{self.current_question_index}\"\n",
    "        query = self.dataset[qidx][index]\n",
    "\n",
    "        if not self.is_api:\n",
    "            if not self.is_jais:\n",
    "                return query + \"\\nAnswer:\" if self.fewshot > 0 else query\n",
    "            elif self.language == \"ar\":\n",
    "                return jais_prompt_ar.format(Question=query)\n",
    "            else:\n",
    "                return jais_prompt_en.format(Question=query)\n",
    "\n",
    "        persona = self.persona_qid[qidx][index]\n",
    "        qinfo = self.question_info[qidx][index]\n",
    "        payload = {\"role\": \"user\", \"content\": f\"{query}\"}\n",
    "        return payload, persona, qinfo\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[f\"Q{self.current_question_index}\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    language = \"en\"\n",
    "    country = \"us\"\n",
    "    # model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "    # model_name = \"AceGPT-13B-chat\"\n",
    "    model_name = \"bigscience/mt0-xxl\"\n",
    "    # model_name = 'gpt-3.5'\n",
    "    model_name = model_name.split(\"/\")[-1]\n",
    "\n",
    "    filepath = f\"../dataset/wvs_template.{language}.yml\"\n",
    "    dataset = WVSDataset(filepath,\n",
    "        language=language,\n",
    "        country=country,\n",
    "        fewshot=0,\n",
    "        model_name=model_name,\n",
    "        use_anthro_prompt=False,\n",
    "        api=False,\n",
    "    )\n",
    "\n",
    "    print(len(dataset.question_ids))\n",
    "    dataset.set_question(index=2)\n",
    "    print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZsMJyydOVZu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFowzk-5DJfn"
   },
   "source": [
    "### Querying model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IaNR9TzXQSf"
   },
   "source": [
    "#### Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9dnITc846-4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "f7b6c73f220d4b7cacd677b498ad938f",
      "9dc4212c940a4644b19e6faabbbc2b1c",
      "4a099b0c87c14314a73f19317ec86c64",
      "0823bf77e3624c3cbb82b101ee8164d8",
      "b3e4be6f7f40462aa81038b9a3513be2",
      "b9ef294470274281a4d82c5008334aaa",
      "861f3906007d406280db290ba4e63064",
      "1e90019856c74f3a90d0d5b1e4050315",
      "b4386ac0bf244f66a351b2e05661a30e",
      "170f73038b75456da3756615b0ef6f81",
      "9e97bf4d74b74133bede79b38ffa970d",
      "95427f7e2934437ba5f67b74a9eba707",
      "76908b695c924ccface3d586b52bb51e",
      "a2ee8ae07d54408995b713f9851302d4",
      "8911f383faf647cfb9df816e666416a0",
      "ae866de122654a3db1d478a619befb88",
      "9b404bfb2bb14544ae4a62386e630638",
      "d1b5b097182b4ffe9311aeca2370bf1d",
      "0ee4a38cca684ae8866704a1c3bd4ac2",
      "9f1079e7617c446681d3b47eb0db4a8d"
     ]
    },
    "id": "cvBh3zEYv0WN",
    "outputId": "4cb86187-4e16-4f3c-f7b2-6bcd3f6d2a4a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b6c73f220d4b7cacd677b498ad938f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POY1Pl75KOjs",
    "outputId": "2475e486-a7ec-4a14-a9cd-918aa9d49c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m863.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bitsandbytes-0.47.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "np_PyYLCjDo3"
   },
   "outputs": [],
   "source": [
    "# baseline 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZ5mkH_G3Lvy",
    "outputId": "b678983d-b728-477b-e34d-e008b215cdc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 Personas\n",
      "Language=en | Temperature=0.7 | Tokens=4 | N=5 | Batch=1 | Version=1\n",
      "> Device cuda:0\n",
      "> Running 31 Qs\n",
      "config.json: 100% 1.16k/1.16k [00:00<00:00, 8.20MB/s]\n",
      "2025-08-11 18:44:28.627982: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754937868.862995    3280 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754937868.926143    3280 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754937869.409053    3280 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754937869.409092    3280 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754937869.409096    3280 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754937869.409101    3280 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-11 18:44:29.448315: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "model.safetensors.index.json: 100% 76.5k/76.5k [00:00<00:00, 5.83MB/s]\n",
      "Fetching 82 files:   0% 0/82 [00:00<?, ?it/s]\n",
      "model-00003-of-00082.safetensors:   0% 0.00/194M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model-00002-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00007-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00005-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00008-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00082.safetensors:   0% 0.00/194M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00006-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00001-of-00082.safetensors:   0% 0.00/328M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00006-of-00082.safetensors:   0% 683k/142M [00:00<03:21, 698kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00003-of-00082.safetensors:   0% 184k/194M [00:01<19:13, 168kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00006-of-00082.safetensors:   5% 7.60M/142M [00:01<00:18, 7.39MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00008-of-00082.safetensors:   2% 2.26M/142M [00:01<01:38, 1.42MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00003-of-00082.safetensors:   4% 6.99M/194M [00:01<00:37, 5.04MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00082.safetensors:   1% 2.14M/194M [00:01<02:33, 1.25MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00007-of-00082.safetensors:   3% 4.54M/176M [00:01<01:05, 2.61MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00007-of-00082.safetensors:   5% 9.40M/176M [00:01<00:29, 5.72MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00007-of-00082.safetensors:   9% 16.4M/176M [00:02<00:17, 8.93MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00001-of-00082.safetensors:   3% 8.34M/328M [00:02<01:33, 3.41MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00005-of-00082.safetensors:   5% 8.46M/176M [00:02<00:54, 3.10MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00082.safetensors:  13% 22.1M/176M [00:02<00:19, 7.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00001-of-00082.safetensors:   5% 17.8M/328M [00:02<00:40, 7.74MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00003-of-00082.safetensors:   9% 18.2M/194M [00:03<00:26, 6.72MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00005-of-00082.safetensors:  24% 41.6M/176M [00:03<00:09, 14.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00003-of-00082.safetensors:  17% 32.9M/194M [00:03<00:13, 11.8MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00008-of-00082.safetensors:  13% 19.0M/142M [00:03<00:23, 5.23MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00082.safetensors:  10% 19.2M/194M [00:07<01:08, 2.54MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00082.safetensors:  37% 64.3M/176M [00:07<00:13, 8.17MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00007-of-00082.safetensors:  24% 42.5M/176M [00:08<00:28, 4.69MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00008-of-00082.safetensors:  53% 74.6M/142M [00:08<00:07, 9.30MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00003-of-00082.safetensors:  30% 58.8M/194M [00:08<00:21, 6.41MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00082.safetensors:  23% 45.2M/194M [00:09<00:24, 6.06MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00082.safetensors:  37% 71.4M/194M [00:09<00:10, 11.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00003-of-00082.safetensors:  45% 87.7M/194M [00:09<00:10, 10.4MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00007-of-00082.safetensors:  62% 109M/176M [00:10<00:04, 14.0MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00005-of-00082.safetensors:  62% 109M/176M [00:10<00:06, 10.2MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00003-of-00082.safetensors:  67% 130M/194M [00:11<00:03, 16.3MB/s] \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00001-of-00082.safetensors:  18% 59.4M/328M [00:11<00:50, 5.30MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00082.safetensors:  62% 109M/176M [00:12<00:07, 9.33MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00001-of-00082.safetensors:  39% 126M/328M [00:11<00:14, 13.4MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00082.safetensors:  65% 127M/194M [00:13<00:05, 13.2MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00007-of-00082.safetensors: 100% 176M/176M [00:13<00:00, 13.4MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model-00009-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00008-of-00082.safetensors: 100% 142M/142M [00:14<00:00, 10.1MB/s]\n",
      "\n",
      "model-00003-of-00082.safetensors: 100% 194M/194M [00:14<00:00, 13.7MB/s]\n",
      "\n",
      "model-00010-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00011-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00005-of-00082.safetensors: 100% 176M/176M [00:15<00:00, 11.6MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00006-of-00082.safetensors:  53% 74.7M/142M [00:15<00:13, 4.93MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00012-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00001-of-00082.safetensors:  59% 194M/328M [00:16<00:09, 14.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00082.safetensors: 100% 194M/194M [00:16<00:00, 16.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00001-of-00082.safetensors:  80% 261M/328M [00:19<00:03, 17.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00004-of-00082.safetensors: 100% 194M/194M [00:19<00:00, 10.1MB/s]\n",
      "model-00002-of-00082.safetensors: 100% 176M/176M [00:19<00:00, 9.13MB/s]\n",
      "\n",
      "\n",
      "model-00014-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00013-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00001-of-00082.safetensors: 100% 328M/328M [00:19<00:00, 16.7MB/s]\n",
      "Fetching 82 files:   1% 1/82 [00:20<27:03, 20.04s/it]\n",
      "\n",
      "\n",
      "model-00009-of-00082.safetensors:   0% 310k/176M [00:06<1:00:10, 48.6kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00011-of-00082.safetensors:   0% 72.1k/176M [00:05<3:41:10, 13.2kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00006-of-00082.safetensors: 100% 142M/142M [00:19<00:00, 7.14MB/s]\n",
      "Fetching 82 files:   7% 6/82 [00:20<03:09,  2.49s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00015-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00016-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00009-of-00082.safetensors:   5% 8.35M/176M [00:06<01:40, 1.67MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00012-of-00082.safetensors:   0% 274k/142M [00:04<42:14, 55.8kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00013-of-00082.safetensors:   0% 280k/176M [00:00<08:24, 348kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00014-of-00082.safetensors:   1% 1.19M/142M [00:00<01:46, 1.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00011-of-00082.safetensors:  23% 41.0M/176M [00:06<00:15, 8.59MB/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00016-of-00082.safetensors:   0% 242k/142M [00:00<07:09, 329kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00009-of-00082.safetensors:  18% 30.9M/176M [00:07<00:22, 6.55MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00014-of-00082.safetensors:   7% 9.49M/142M [00:01<00:18, 6.98MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00009-of-00082.safetensors:  34% 60.6M/176M [00:08<00:09, 12.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00013-of-00082.safetensors:   7% 12.1M/176M [00:02<00:34, 4.75MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00011-of-00082.safetensors:  56% 98.1M/176M [00:08<00:04, 17.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00015-of-00082.safetensors:  10% 16.8M/176M [00:02<00:26, 5.98MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00012-of-00082.safetensors:  14% 19.5M/142M [00:07<00:37, 3.25MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00011-of-00082.safetensors:  73% 128M/176M [00:08<00:02, 21.4MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00010-of-00082.safetensors:  26% 37.4M/142M [00:09<00:25, 4.06MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00015-of-00082.safetensors:  21% 37.6M/176M [00:03<00:11, 11.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00015-of-00082.safetensors:  38% 66.6M/176M [00:04<00:05, 20.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00009-of-00082.safetensors:  62% 109M/176M [00:14<00:06, 10.2MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00011-of-00082.safetensors: 100% 176M/176M [00:13<00:00, 13.4MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00017-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00010-of-00082.safetensors:  56% 79.2M/142M [00:13<00:09, 6.27MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00016-of-00082.safetensors:  17% 24.5M/142M [00:07<00:36, 3.18MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00015-of-00082.safetensors:  62% 109M/176M [00:08<00:05, 13.2MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00010-of-00082.safetensors: 100% 142M/142M [00:14<00:00, 9.67MB/s]\n",
      "\n",
      "model-00018-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00009-of-00082.safetensors: 100% 176M/176M [00:16<00:00, 10.8MB/s]\n",
      "Fetching 82 files:  11% 9/82 [00:29<03:25,  2.81s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00013-of-00082.safetensors:  29% 51.1M/176M [00:10<00:23, 5.21MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00019-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00016-of-00082.safetensors:  53% 74.6M/142M [00:10<00:07, 8.54MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00012-of-00082.safetensors:  53% 74.6M/142M [00:15<00:11, 5.80MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00017-of-00082.safetensors:   1% 2.63M/176M [00:02<03:05, 934kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00013-of-00082.safetensors:  63% 110M/176M [00:11<00:05, 11.6MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00015-of-00082.safetensors: 100% 176M/176M [00:13<00:00, 12.9MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00012-of-00082.safetensors: 100% 142M/142M [00:18<00:00, 10.1MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00012-of-00082.safetensors: 100% 142M/142M [00:18<00:00, 7.76MB/s]\n",
      "Fetching 82 files:  15% 12/82 [00:34<02:38,  2.26s/it]\n",
      "\n",
      "\n",
      "\n",
      "model-00020-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00021-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00016-of-00082.safetensors: 100% 142M/142M [00:14<00:00, 10.0MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00022-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00014-of-00082.safetensors:  54% 76.6M/142M [00:15<00:12, 5.05MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00014-of-00082.safetensors: 100% 142M/142M [00:15<00:00, 9.09MB/s]\n",
      "\n",
      "\n",
      "model-00023-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00013-of-00082.safetensors: 100% 176M/176M [00:16<00:00, 10.9MB/s]\n",
      "Fetching 82 files:  16% 13/82 [00:35<02:33,  2.22s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00024-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00017-of-00082.safetensors:  18% 31.5M/176M [00:08<00:35, 4.09MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00022-of-00082.safetensors:   0% 63.7k/142M [00:01<1:09:08, 34.1kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00018-of-00082.safetensors:  23% 32.2M/142M [00:07<00:22, 4.93MB/s] \u001b[A\n",
      "\n",
      "\n",
      "model-00019-of-00082.safetensors:  10% 16.8M/176M [00:07<01:07, 2.35MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00023-of-00082.safetensors:   0% 32.9k/176M [00:01<2:13:55, 21.9kB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00023-of-00082.safetensors:   0% 261k/176M [00:01<14:02, 208kB/s]    \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00019-of-00082.safetensors:  33% 57.2M/176M [00:07<00:12, 9.32MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00021-of-00082.safetensors:   4% 6.40M/176M [00:03<01:41, 1.66MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00021-of-00082.safetensors:   8% 14.7M/176M [00:04<00:36, 4.46MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00019-of-00082.safetensors:  71% 124M/176M [00:08<00:02, 21.4MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00017-of-00082.safetensors:  62% 109M/176M [00:11<00:05, 12.4MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00020-of-00082.safetensors:  10% 13.7M/142M [00:05<00:47, 2.68MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00020-of-00082.safetensors:  23% 32.7M/142M [00:05<00:15, 6.90MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00024-of-00082.safetensors:  12% 16.8M/142M [00:03<00:27, 4.51MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00021-of-00082.safetensors:  20% 35.9M/176M [00:05<00:17, 7.93MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00024-of-00082.safetensors:  26% 36.2M/142M [00:04<00:10, 10.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00018-of-00082.safetensors:  53% 74.6M/142M [00:14<00:11, 5.66MB/s]\u001b[A\n",
      "model-00018-of-00082.safetensors: 100% 142M/142M [00:15<00:00, 9.44MB/s]\n",
      "\n",
      "model-00025-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00021-of-00082.safetensors:  37% 65.2M/176M [00:13<00:23, 4.66MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00023-of-00082.safetensors:  17% 30.3M/176M [00:12<00:56, 2.60MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00022-of-00082.safetensors:  26% 37.1M/142M [00:13<00:37, 2.81MB/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00022-of-00082.safetensors:  53% 74.6M/142M [00:14<00:10, 6.54MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00017-of-00082.safetensors: 100% 176M/176M [00:21<00:00, 8.26MB/s]\n",
      "Fetching 82 files:  21% 17/82 [00:49<02:58,  2.75s/it]\n",
      "model-00025-of-00082.safetensors:   0% 19.2k/176M [00:04<12:11:47, 4.00kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00026-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00019-of-00082.safetensors: 100% 176M/176M [00:19<00:00, 8.92MB/s]\n",
      "Fetching 82 files:  23% 19/82 [00:49<02:13,  2.12s/it]\n",
      "model-00025-of-00082.safetensors:   0% 284k/176M [00:05<40:05, 72.9kB/s]    \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00020-of-00082.safetensors:  53% 74.6M/142M [00:15<00:14, 4.70MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00027-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00024-of-00082.safetensors:  53% 74.6M/142M [00:15<00:14, 4.57MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00025-of-00082.safetensors:   7% 12.2M/176M [00:07<01:08, 2.38MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00021-of-00082.safetensors:  62% 109M/176M [00:18<00:09, 6.77MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00026-of-00082.safetensors:   0% 37.7k/142M [00:02<2:54:52, 13.5kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00026-of-00082.safetensors:   0% 252k/142M [00:02<20:07, 117kB/s]    \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00023-of-00082.safetensors:  40% 71.0M/176M [00:16<00:21, 4.94MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00026-of-00082.safetensors:   1% 1.57M/142M [00:03<02:34, 909kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00023-of-00082.safetensors:  67% 119M/176M [00:17<00:05, 9.65MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00022-of-00082.safetensors: 100% 142M/142M [00:18<00:00, 7.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00028-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00024-of-00082.safetensors: 100% 142M/142M [00:17<00:00, 7.94MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model-00027-of-00082.safetensors:   0% 299k/176M [00:03<39:06, 74.8kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00029-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00023-of-00082.safetensors: 100% 176M/176M [00:22<00:00, 7.76MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00021-of-00082.safetensors: 100% 176M/176M [00:24<00:00, 7.26MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00026-of-00082.safetensors:   5% 7.50M/142M [00:08<02:14, 1.00MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00030-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "model-00025-of-00082.safetensors:  29% 51.3M/176M [00:14<00:26, 4.77MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00031-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00027-of-00082.safetensors:   6% 10.0M/176M [00:12<03:07, 884kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00020-of-00082.safetensors: 100% 142M/142M [00:28<00:00, 4.93MB/s]\n",
      "Fetching 82 files:  24% 20/82 [01:02<03:53,  3.76s/it]\n",
      "\n",
      "\n",
      "model-00027-of-00082.safetensors:  15% 26.7M/176M [00:12<00:51, 2.91MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00028-of-00082.safetensors:   0% 134k/142M [00:09<2:38:24, 14.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00027-of-00082.safetensors:  27% 47.7M/176M [00:12<00:20, 6.34MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00032-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00030-of-00082.safetensors:   0% 12.6k/142M [00:04<15:09:58, 2.59kB/s]\u001b[A\u001b[A\n",
      "model-00025-of-00082.safetensors:  63% 110M/176M [00:18<00:08, 7.78MB/s] \u001b[A\n",
      "\n",
      "model-00030-of-00082.safetensors:   0% 403k/142M [00:05<21:01, 112kB/s]     \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00032-of-00082.safetensors:   0% 142k/142M [00:00<13:24, 176kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00031-of-00082.safetensors:   4% 7.38M/176M [00:05<02:00, 1.39MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00029-of-00082.safetensors:   5% 8.46M/176M [00:14<04:44, 588kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00026-of-00082.safetensors:   5% 7.50M/142M [00:19<02:14, 1.00MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00025-of-00082.safetensors: 100% 176M/176M [00:24<00:00, 7.24MB/s]\n",
      "Fetching 82 files:  30% 25/82 [01:08<02:18,  2.44s/it]\n",
      "model-00033-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00031-of-00082.safetensors:  12% 20.6M/176M [00:10<01:15, 2.07MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00030-of-00082.safetensors:   8% 11.4M/142M [00:10<01:32, 1.41MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00028-of-00082.safetensors:  17% 24.4M/142M [00:15<01:02, 1.88MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00029-of-00082.safetensors:  16% 28.2M/176M [00:15<01:03, 2.31MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00030-of-00082.safetensors:  26% 36.2M/142M [00:11<00:22, 4.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00032-of-00082.safetensors:  16% 22.4M/142M [00:09<00:50, 2.35MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00028-of-00082.safetensors:  53% 74.7M/142M [00:19<00:12, 5.39MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00027-of-00082.safetensors:  27% 47.7M/176M [00:28<00:20, 6.34MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00027-of-00082.safetensors:  65% 115M/176M [00:29<00:13, 4.55MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00026-of-00082.safetensors:  53% 74.6M/142M [00:33<00:27, 2.45MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00027-of-00082.safetensors: 100% 176M/176M [00:33<00:00, 5.29MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00029-of-00082.safetensors:  40% 71.0M/176M [00:29<00:37, 2.82MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00034-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00031-of-00082.safetensors:  41% 71.3M/176M [00:25<00:34, 3.01MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00030-of-00082.safetensors:  26% 36.2M/142M [00:30<00:22, 4.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00028-of-00082.safetensors:  53% 74.7M/142M [00:34<00:12, 5.39MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00032-of-00082.safetensors:  16% 22.4M/142M [00:25<00:50, 2.35MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00026-of-00082.safetensors: 100% 142M/142M [00:39<00:00, 3.58MB/s]\n",
      "Fetching 82 files:  32% 26/82 [01:29<04:25,  4.75s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00029-of-00082.safetensors:  64% 113M/176M [00:34<00:15, 4.07MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00035-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00030-of-00082.safetensors:  58% 82.1M/142M [00:30<00:20, 2.88MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00031-of-00082.safetensors:  66% 116M/176M [00:31<00:13, 4.31MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00034-of-00082.safetensors:   0% 32.1k/142M [00:06<7:58:25, 4.93kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00032-of-00082.safetensors:  53% 74.6M/142M [00:27<00:23, 2.80MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00034-of-00082.safetensors:   0% 201k/142M [00:06<1:00:19, 39.1kB/s] \u001b[A\u001b[A\u001b[A\n",
      "model-00033-of-00082.safetensors:   8% 14.8M/176M [00:21<03:53, 688kB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00034-of-00082.safetensors:   3% 4.33M/142M [00:07<02:04, 1.11MB/s] \u001b[A\u001b[A\u001b[A\n",
      "model-00033-of-00082.safetensors:  21% 37.3M/176M [00:22<01:05, 2.11MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00035-of-00082.safetensors:   0% 396k/176M [00:01<13:11, 221kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00028-of-00082.safetensors: 100% 142M/142M [00:37<00:00, 3.79MB/s]\n",
      "Fetching 82 files:  34% 28/82 [01:31<03:22,  3.75s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00036-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00031-of-00082.safetensors: 100% 176M/176M [00:33<00:00, 5.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00037-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00032-of-00082.safetensors: 100% 142M/142M [00:30<00:00, 5.75MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00030-of-00082.safetensors: 100% 142M/142M [00:34<00:00, 4.08MB/s]\n",
      "model-00032-of-00082.safetensors: 100% 142M/142M [00:30<00:00, 4.68MB/s]\n",
      "\n",
      "\n",
      "model-00038-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00039-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00029-of-00082.safetensors: 100% 176M/176M [00:39<00:00, 4.42MB/s]\n",
      "Fetching 82 files:  35% 29/82 [01:34<03:10,  3.59s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00036-of-00082.safetensors:   0% 157k/142M [00:02<38:44, 60.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00035-of-00082.safetensors:   4% 7.33M/176M [00:04<01:36, 1.74MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00040-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00033-of-00082.safetensors:  45% 79.3M/176M [00:25<00:21, 4.52MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00035-of-00082.safetensors:  12% 21.3M/176M [00:05<00:30, 5.10MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00038-of-00082.safetensors:   0% 22.5k/142M [00:01<2:25:30, 16.2kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00036-of-00082.safetensors:   5% 7.53M/142M [00:03<00:50, 2.64MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00038-of-00082.safetensors:   0% 207k/142M [00:01<12:52, 183kB/s]    \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00035-of-00082.safetensors:  26% 44.9M/176M [00:07<00:17, 7.39MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00033-of-00082.safetensors: 100% 176M/176M [00:30<00:00, 5.79MB/s]\n",
      "Fetching 82 files:  40% 33/82 [01:39<02:03,  2.52s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00037-of-00082.safetensors:   3% 5.59M/176M [00:06<03:30, 810kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00041-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00039-of-00082.safetensors:   6% 9.92M/176M [00:06<01:47, 1.54MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00034-of-00082.safetensors:  53% 74.6M/142M [00:16<00:11, 5.99MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00041-of-00082.safetensors:   0% 657k/176M [00:01<04:53, 597kB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00034-of-00082.safetensors: 100% 142M/142M [00:17<00:00, 8.06MB/s]\n",
      "Fetching 82 files:  41% 34/82 [01:41<01:54,  2.39s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00040-of-00082.safetensors:  12% 16.8M/142M [00:06<00:51, 2.43MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00042-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00035-of-00082.safetensors:  62% 109M/176M [00:13<00:06, 10.4MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00039-of-00082.safetensors:  21% 37.3M/176M [00:08<00:27, 4.96MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00037-of-00082.safetensors:  20% 34.7M/176M [00:10<00:34, 4.12MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00041-of-00082.safetensors:   6% 11.1M/176M [00:03<00:44, 3.70MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00035-of-00082.safetensors: 100% 176M/176M [00:15<00:00, 11.7MB/s]\n",
      "Fetching 82 files:  43% 35/82 [01:44<01:59,  2.55s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00043-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00037-of-00082.safetensors:  38% 66.8M/176M [00:12<00:16, 6.65MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00039-of-00082.safetensors:  40% 71.0M/176M [00:12<00:14, 7.20MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00042-of-00082.safetensors:   0% 147k/142M [00:04<1:08:11, 34.6kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00038-of-00082.safetensors:  25% 35.0M/142M [00:13<00:39, 2.69MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00043-of-00082.safetensors:   0% 142k/176M [00:02<57:14, 51.1kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00036-of-00082.safetensors:   5% 7.53M/142M [00:17<00:50, 2.64MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00037-of-00082.safetensors:  62% 109M/176M [00:16<00:08, 8.26MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00038-of-00082.safetensors:  53% 74.6M/142M [00:16<00:12, 5.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00043-of-00082.safetensors:   2% 3.94M/176M [00:05<03:12, 892kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00042-of-00082.safetensors:   6% 9.00M/142M [00:08<01:47, 1.23MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00039-of-00082.safetensors:  69% 121M/176M [00:20<00:08, 6.60MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00041-of-00082.safetensors:  27% 47.3M/176M [00:13<00:37, 3.44MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00039-of-00082.safetensors: 100% 176M/176M [00:20<00:00, 8.44MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00044-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00043-of-00082.safetensors:  13% 23.5M/176M [00:10<00:54, 2.77MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00040-of-00082.safetensors:  53% 75.4M/142M [00:20<00:17, 3.73MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00036-of-00082.safetensors:  53% 74.6M/142M [00:27<00:23, 2.80MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00037-of-00082.safetensors: 100% 176M/176M [00:31<00:00, 5.63MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00044-of-00082.safetensors:   0% 59.5k/142M [00:09<6:09:44, 6.38kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00045-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00043-of-00082.safetensors:  24% 41.6M/176M [00:20<01:02, 2.14MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00036-of-00082.safetensors: 100% 142M/142M [00:36<00:00, 3.87MB/s]\n",
      "Fetching 82 files:  44% 36/82 [02:08<05:09,  6.72s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00046-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00040-of-00082.safetensors: 100% 142M/142M [00:34<00:00, 4.13MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model-00042-of-00082.safetensors:   6% 9.00M/142M [00:27<01:47, 1.23MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00041-of-00082.safetensors:  27% 47.3M/176M [00:28<00:37, 3.44MB/s]\u001b[A\n",
      "\n",
      "model-00038-of-00082.safetensors:  53% 74.6M/142M [00:35<00:12, 5.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00047-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00038-of-00082.safetensors: 100% 142M/142M [00:36<00:00, 3.90MB/s]\n",
      "Fetching 82 files:  46% 38/82 [02:09<03:23,  4.62s/it]\n",
      "\n",
      "model-00048-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "model-00041-of-00082.safetensors:  62% 109M/176M [00:34<00:21, 3.14MB/s] \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00046-of-00082.safetensors:   0% 145k/142M [00:06<1:38:20, 24.0kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00043-of-00082.safetensors:  62% 109M/176M [00:29<00:15, 4.44MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00044-of-00082.safetensors:  19% 27.2M/142M [00:19<01:12, 1.59MB/s]  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00047-of-00082.safetensors:   0% 466k/176M [00:05<36:35, 79.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00041-of-00082.safetensors: 100% 176M/176M [00:38<00:00, 4.52MB/s]\n",
      "Fetching 82 files:  50% 41/82 [02:18<02:39,  3.89s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00045-of-00082.safetensors:   5% 8.44M/176M [00:14<04:47, 581kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00043-of-00082.safetensors: 100% 176M/176M [00:34<00:00, 5.14MB/s]\n",
      "\n",
      "\n",
      "model-00048-of-00082.safetensors:   0% 264k/142M [00:08<1:17:41, 30.3kB/s]\u001b[A\u001b[A\n",
      "model-00049-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00050-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00042-of-00082.safetensors:  53% 74.6M/142M [00:38<00:32, 2.07MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00048-of-00082.safetensors:   8% 11.3M/142M [00:09<01:20, 1.62MB/s] \u001b[A\u001b[A\n",
      "model-00049-of-00082.safetensors:   0% 50.9k/176M [00:00<45:30, 64.4kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00050-of-00082.safetensors:   0% 281k/142M [00:00<06:03, 389kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00045-of-00082.safetensors:  18% 31.0M/176M [00:18<01:10, 2.05MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00049-of-00082.safetensors:   2% 3.93M/176M [00:03<02:34, 1.11MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00047-of-00082.safetensors:  13% 22.3M/176M [00:14<01:26, 1.78MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00042-of-00082.safetensors: 100% 142M/142M [00:41<00:00, 3.40MB/s]\n",
      "Fetching 82 files:  51% 42/82 [02:23<02:39,  3.98s/it]\n",
      "model-00049-of-00082.safetensors:   6% 11.0M/176M [00:04<00:51, 3.21MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00051-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00045-of-00082.safetensors:  40% 71.0M/176M [00:19<00:19, 5.48MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00050-of-00082.safetensors:  10% 14.7M/142M [00:04<00:41, 3.09MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00046-of-00082.safetensors:  21% 29.3M/142M [00:15<00:54, 2.07MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00051-of-00082.safetensors:   0% 370k/176M [00:00<06:15, 468kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00045-of-00082.safetensors:  62% 109M/176M [00:20<00:07, 9.36MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00044-of-00082.safetensors:  67% 94.3M/142M [00:30<00:12, 3.78MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00046-of-00082.safetensors:  53% 74.6M/142M [00:20<00:14, 4.61MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00051-of-00082.safetensors:   7% 12.8M/176M [00:05<01:04, 2.52MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00044-of-00082.safetensors: 100% 142M/142M [00:34<00:00, 4.15MB/s]\n",
      "Fetching 82 files:  54% 44/82 [02:28<02:17,  3.61s/it]\n",
      "\n",
      "\n",
      "\n",
      "model-00052-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00049-of-00082.safetensors:  24% 41.6M/176M [00:13<00:41, 3.20MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00047-of-00082.safetensors:  40% 71.1M/176M [00:24<00:30, 3.43MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00047-of-00082.safetensors: 100% 176M/176M [00:25<00:00, 7.00MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model-00051-of-00082.safetensors:  18% 31.0M/176M [00:10<00:47, 3.03MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00053-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00045-of-00082.safetensors:  62% 109M/176M [00:34<00:07, 9.36MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00050-of-00082.safetensors:  10% 14.7M/142M [00:19<00:41, 3.09MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00046-of-00082.safetensors:  53% 74.6M/142M [00:30<00:14, 4.61MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00048-of-00082.safetensors:   8% 11.3M/142M [00:28<01:20, 1.62MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00048-of-00082.safetensors:  55% 78.3M/142M [00:28<00:20, 3.12MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00052-of-00082.safetensors:   0% 46.4k/142M [00:09<8:13:11, 4.79kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00045-of-00082.safetensors: 100% 176M/176M [00:34<00:00, 5.05MB/s]\n",
      "Fetching 82 files:  55% 45/82 [02:39<03:00,  4.88s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00054-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00051-of-00082.safetensors:  32% 56.2M/176M [00:15<00:31, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00050-of-00082.safetensors:  53% 74.6M/142M [00:23<00:21, 3.16MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00046-of-00082.safetensors: 100% 142M/142M [00:35<00:00, 4.01MB/s]\n",
      "Fetching 82 files:  56% 46/82 [02:43<02:52,  4.79s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00053-of-00082.safetensors:   0% 259k/176M [00:09<1:44:04, 28.1kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00048-of-00082.safetensors: 100% 142M/142M [00:33<00:00, 4.19MB/s]\n",
      "Fetching 82 files:  59% 48/82 [02:44<01:43,  3.06s/it]\n",
      "\n",
      "model-00055-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00056-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00051-of-00082.safetensors:  62% 109M/176M [00:20<00:10, 6.36MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00050-of-00082.safetensors: 100% 142M/142M [00:26<00:00, 5.42MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00054-of-00082.safetensors:   0% 19.4k/142M [00:05<11:57:51, 3.29kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00052-of-00082.safetensors:  30% 41.9M/142M [00:15<00:31, 3.20MB/s]  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00054-of-00082.safetensors:   0% 183k/142M [00:06<56:53, 41.4kB/s]    \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00057-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00052-of-00082.safetensors:  53% 74.6M/142M [00:16<00:10, 6.54MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00053-of-00082.safetensors:  21% 36.7M/176M [00:11<00:34, 4.03MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00049-of-00082.safetensors:  62% 109M/176M [00:27<00:15, 4.29MB/s] \u001b[A\n",
      "\n",
      "model-00055-of-00082.safetensors:   0% 10.2k/176M [00:02<11:18:53, 4.31kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00056-of-00082.safetensors:   0% 224k/142M [00:02<23:28, 100kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00056-of-00082.safetensors:   2% 2.14M/142M [00:02<01:58, 1.18MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00057-of-00082.safetensors:   0% 39.9k/176M [00:01<1:42:18, 28.6kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00049-of-00082.safetensors: 100% 176M/176M [00:28<00:00, 6.26MB/s]\n",
      "Fetching 82 files:  60% 49/82 [02:47<01:40,  3.04s/it]\n",
      "model-00058-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00053-of-00082.safetensors:  40% 71.0M/176M [00:12<00:13, 7.97MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00054-of-00082.safetensors:  17% 23.8M/142M [00:08<00:26, 4.51MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00056-of-00082.safetensors:   5% 7.56M/142M [00:03<00:54, 2.48MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00051-of-00082.safetensors: 100% 176M/176M [00:24<00:00, 7.06MB/s]\n",
      "Fetching 82 files:  62% 51/82 [02:48<01:06,  2.15s/it]\n",
      "\n",
      "model-00055-of-00082.safetensors:   9% 16.4M/176M [00:04<00:36, 4.42MB/s]   \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00057-of-00082.safetensors:   3% 5.97M/176M [00:02<01:13, 2.32MB/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00059-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00057-of-00082.safetensors:  10% 18.3M/176M [00:05<00:41, 3.77MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00052-of-00082.safetensors: 100% 142M/142M [00:21<00:00, 6.50MB/s]\n",
      "Fetching 82 files:  63% 52/82 [02:51<01:08,  2.27s/it]\n",
      "model-00058-of-00082.safetensors:   0% 78.6k/142M [00:03<1:57:14, 20.1kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00060-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00053-of-00082.safetensors:  65% 114M/176M [00:17<00:06, 9.08MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00059-of-00082.safetensors:   0% 48.9k/176M [00:03<3:05:42, 15.8kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00054-of-00082.safetensors:  53% 74.6M/142M [00:15<00:11, 6.00MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00059-of-00082.safetensors:   4% 7.36M/176M [00:06<02:11, 1.28MB/s]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00060-of-00082.safetensors:   0% 54.1k/142M [00:03<2:50:46, 13.8kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00053-of-00082.safetensors: 100% 176M/176M [00:21<00:00, 8.37MB/s]\n",
      "Fetching 82 files:  65% 53/82 [02:55<01:19,  2.74s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00061-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00057-of-00082.safetensors:  25% 43.6M/176M [00:10<00:27, 4.85MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00058-of-00082.safetensors:  14% 20.1M/142M [00:08<00:44, 2.72MB/s]  \u001b[A\n",
      "\n",
      "model-00055-of-00082.safetensors:  34% 59.7M/176M [00:12<00:22, 5.25MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00056-of-00082.safetensors:   5% 7.56M/142M [00:14<00:54, 2.48MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00059-of-00082.safetensors:  18% 31.0M/176M [00:10<00:39, 3.62MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00055-of-00082.safetensors:  65% 114M/176M [00:15<00:06, 8.86MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00059-of-00082.safetensors:  35% 61.7M/176M [00:11<00:13, 8.40MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00055-of-00082.safetensors: 100% 176M/176M [00:19<00:00, 8.97MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00056-of-00082.safetensors:  53% 74.6M/142M [00:19<00:16, 4.02MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00062-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00061-of-00082.safetensors:   3% 5.87M/176M [00:08<04:16, 663kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00054-of-00082.safetensors: 100% 142M/142M [00:25<00:00, 5.62MB/s]\n",
      "Fetching 82 files:  66% 54/82 [03:04<02:01,  4.35s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00063-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00061-of-00082.safetensors:  11% 19.5M/176M [00:09<00:58, 2.69MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00062-of-00082.safetensors:   0% 272k/142M [00:00<07:48, 302kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00061-of-00082.safetensors:  22% 38.1M/176M [00:09<00:23, 5.96MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00062-of-00082.safetensors:   1% 1.79M/142M [00:01<01:24, 1.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00057-of-00082.safetensors:  25% 43.6M/176M [00:23<00:27, 4.85MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00058-of-00082.safetensors:  14% 20.1M/142M [00:21<00:44, 2.72MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00060-of-00082.safetensors:   0% 265k/142M [00:17<2:50:31, 13.8kB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00060-of-00082.safetensors:  26% 36.5M/142M [00:18<00:48, 2.15MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00057-of-00082.safetensors:  62% 109M/176M [00:24<00:14, 4.71MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00060-of-00082.safetensors:  53% 74.6M/142M [00:18<00:12, 5.19MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00062-of-00082.safetensors:   5% 7.51M/142M [00:05<01:43, 1.30MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00061-of-00082.safetensors:  38% 66.9M/176M [00:15<00:19, 5.61MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00056-of-00082.safetensors: 100% 142M/142M [00:28<00:00, 4.98MB/s]\n",
      "Fetching 82 files:  68% 56/82 [03:12<01:49,  4.22s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00064-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00059-of-00082.safetensors:  62% 109M/176M [00:25<00:15, 4.46MB/s] \u001b[A\u001b[A\u001b[A\n",
      "model-00058-of-00082.safetensors:  53% 74.6M/142M [00:26<00:22, 2.93MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00063-of-00082.safetensors:   8% 13.9M/176M [00:09<01:50, 1.47MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00057-of-00082.safetensors: 100% 176M/176M [00:29<00:00, 5.95MB/s]\n",
      "Fetching 82 files:  70% 57/82 [03:15<01:35,  3.80s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00061-of-00082.safetensors:  62% 109M/176M [00:19<00:09, 7.30MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00063-of-00082.safetensors:  13% 22.3M/176M [00:10<01:03, 2.43MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00065-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00063-of-00082.safetensors:  26% 46.3M/176M [00:14<00:34, 3.70MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00058-of-00082.safetensors: 100% 142M/142M [00:32<00:00, 4.33MB/s]\n",
      "Fetching 82 files:  71% 58/82 [03:20<01:38,  4.09s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00064-of-00082.safetensors:   6% 8.40M/142M [00:07<01:52, 1.19MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00066-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00059-of-00082.safetensors: 100% 176M/176M [00:35<00:00, 4.93MB/s]\n",
      "Fetching 82 files:  72% 59/82 [03:24<01:35,  4.15s/it]\n",
      "\n",
      "\n",
      "\n",
      "model-00060-of-00082.safetensors: 100% 142M/142M [00:32<00:00, 4.31MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model-00068-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00064-of-00082.safetensors:  22% 31.6M/142M [00:11<00:35, 3.13MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00067-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00061-of-00082.safetensors:  62% 109M/176M [00:33<00:09, 7.30MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00062-of-00082.safetensors:   5% 7.51M/142M [00:24<01:43, 1.30MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00061-of-00082.safetensors: 100% 176M/176M [00:33<00:00, 5.30MB/s]\n",
      "Fetching 82 files:  74% 61/82 [03:28<01:09,  3.33s/it]\n",
      "\n",
      "model-00062-of-00082.safetensors:  53% 74.6M/142M [00:25<00:21, 3.14MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00069-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00063-of-00082.safetensors:  62% 109M/176M [00:25<00:12, 5.19MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00064-of-00082.safetensors:  53% 74.6M/142M [00:16<00:12, 5.32MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00067-of-00082.safetensors:   0% 207k/176M [00:05<1:15:44, 38.6kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00063-of-00082.safetensors: 100% 176M/176M [00:29<00:00, 6.00MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model-00068-of-00082.safetensors:   0% 197k/142M [00:09<1:53:53, 20.7kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00062-of-00082.safetensors: 100% 142M/142M [00:30<00:00, 4.72MB/s]\n",
      "Fetching 82 files:  76% 62/82 [03:34<01:16,  3.82s/it]\n",
      "\n",
      "\n",
      "model-00068-of-00082.safetensors:   0% 629k/142M [00:09<28:11, 83.4kB/s]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00068-of-00082.safetensors:   5% 7.59M/142M [00:09<01:30, 1.48MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00070-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "model-00066-of-00082.safetensors:  18% 25.0M/142M [00:14<01:07, 1.73MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00071-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00064-of-00082.safetensors: 100% 142M/142M [00:25<00:00, 5.61MB/s]\n",
      "Fetching 82 files:  78% 64/82 [03:38<00:56,  3.14s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00065-of-00082.safetensors:  24% 41.7M/176M [00:23<01:14, 1.80MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00069-of-00082.safetensors:   4% 6.48M/176M [00:09<04:04, 692kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00072-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00067-of-00082.safetensors:  10% 17.9M/176M [00:14<01:57, 1.34MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00069-of-00082.safetensors:   8% 14.9M/176M [00:10<01:34, 1.71MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00070-of-00082.safetensors:   0% 312k/142M [00:05<38:29, 61.2kB/s]\u001b[A\u001b[A\n",
      "model-00066-of-00082.safetensors:  53% 75.4M/142M [00:20<00:15, 4.29MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00072-of-00082.safetensors:   0% 47.8k/142M [00:01<1:17:51, 30.3kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00071-of-00082.safetensors:   5% 8.57M/176M [00:05<01:50, 1.51MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00069-of-00082.safetensors:  24% 41.7M/176M [00:15<00:37, 3.59MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00065-of-00082.safetensors:  62% 109M/176M [00:29<00:15, 4.30MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00065-of-00082.safetensors: 100% 176M/176M [00:33<00:00, 5.31MB/s]\n",
      "Fetching 82 files:  79% 65/82 [03:48<01:19,  4.68s/it]\n",
      "\n",
      "\n",
      "model-00068-of-00082.safetensors:  53% 74.7M/142M [00:23<00:16, 4.11MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00071-of-00082.safetensors:  13% 22.6M/176M [00:13<01:33, 1.64MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00073-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00070-of-00082.safetensors:  13% 18.7M/142M [00:19<02:01, 1.01MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00067-of-00082.safetensors:  36% 62.7M/176M [00:29<00:47, 2.36MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00071-of-00082.safetensors:  18% 31.1M/176M [00:19<01:33, 1.55MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00069-of-00082.safetensors:  62% 109M/176M [00:29<00:15, 4.32MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00066-of-00082.safetensors:  53% 75.4M/142M [00:38<00:15, 4.29MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00068-of-00082.safetensors:  53% 74.7M/142M [00:34<00:16, 4.11MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00072-of-00082.safetensors:   0% 174k/142M [00:19<1:17:47, 30.3kB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00067-of-00082.safetensors:  66% 116M/176M [00:34<00:13, 4.36MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00070-of-00082.safetensors:  53% 74.6M/142M [00:24<00:17, 3.90MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00071-of-00082.safetensors:  34% 60.2M/176M [00:30<00:52, 2.20MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00067-of-00082.safetensors: 100% 176M/176M [00:40<00:00, 4.37MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00073-of-00082.safetensors:   7% 12.2M/176M [00:16<03:38, 748kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00074-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00066-of-00082.safetensors: 100% 142M/142M [00:45<00:00, 3.12MB/s]\n",
      "Fetching 82 files:  80% 66/82 [04:05<02:03,  7.69s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00072-of-00082.safetensors:  27% 38.0M/142M [00:27<01:12, 1.43MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00075-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00072-of-00082.safetensors:  53% 74.6M/142M [00:27<00:20, 3.28MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00068-of-00082.safetensors: 100% 142M/142M [00:42<00:00, 3.37MB/s]\n",
      "Fetching 82 files:  83% 68/82 [04:06<01:06,  4.75s/it]\n",
      "\n",
      "\n",
      "\n",
      "model-00074-of-00082.safetensors:   0% 95.5k/142M [00:01<38:16, 61.7kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00076-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00071-of-00082.safetensors:  62% 109M/176M [00:33<00:14, 4.61MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00075-of-00082.safetensors:   0% 382k/176M [00:01<15:10, 193kB/s]\u001b[A\n",
      "\n",
      "model-00070-of-00082.safetensors: 100% 142M/142M [00:33<00:00, 4.19MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00069-of-00082.safetensors:  62% 109M/176M [00:39<00:15, 4.32MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00077-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00069-of-00082.safetensors: 100% 176M/176M [00:44<00:00, 3.99MB/s]\n",
      "Fetching 82 files:  84% 69/82 [04:13<01:07,  5.19s/it]\n",
      "\n",
      "\n",
      "model-00076-of-00082.safetensors:   1% 1.56M/142M [00:06<09:45, 239kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00078-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00072-of-00082.safetensors: 100% 142M/142M [00:34<00:00, 4.06MB/s]\n",
      "\n",
      "model-00075-of-00082.safetensors:   7% 12.6M/176M [00:07<01:33, 1.75MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:   0% 0.00/176M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00073-of-00082.safetensors:  40% 71.0M/176M [00:25<00:32, 3.24MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00071-of-00082.safetensors: 100% 176M/176M [00:40<00:00, 4.39MB/s]\n",
      "Fetching 82 files:  87% 71/82 [04:14<00:37,  3.43s/it]\n",
      "\n",
      "\n",
      "\n",
      "model-00074-of-00082.safetensors:  14% 19.9M/142M [00:09<00:56, 2.16MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00077-of-00082.safetensors:   0% 77.1k/176M [00:06<3:59:45, 12.2kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00073-of-00082.safetensors:  67% 118M/176M [00:26<00:09, 6.18MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00077-of-00082.safetensors:   1% 1.77M/176M [00:06<07:30, 387kB/s]   \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00080-of-00082.safetensors:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00077-of-00082.safetensors:   3% 4.72M/176M [00:06<02:15, 1.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00077-of-00082.safetensors:   4% 6.26M/176M [00:06<01:34, 1.79MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00077-of-00082.safetensors:   5% 8.38M/176M [00:10<02:52, 968kB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00076-of-00082.safetensors:  13% 18.3M/142M [00:12<01:10, 1.74MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00075-of-00082.safetensors:  30% 52.0M/176M [00:13<00:27, 4.56MB/s]\u001b[A\n",
      "\n",
      "model-00077-of-00082.safetensors:   6% 11.2M/176M [00:10<01:40, 1.64MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00073-of-00082.safetensors: 100% 176M/176M [00:30<00:00, 5.71MB/s]\n",
      "\n",
      "\n",
      "Fetching 82 files:  89% 73/82 [04:19<00:27,  3.05s/it]\n",
      "\n",
      "model-00077-of-00082.safetensors:   9% 15.8M/176M [00:10<00:46, 3.46MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:   0% 102k/176M [00:05<2:41:01, 18.2kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:   1% 1.25M/176M [00:05<09:34, 304kB/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00077-of-00082.safetensors:  12% 20.9M/176M [00:10<00:24, 6.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00081-of-00082.safetensors:   0% 0.00/328M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:   3% 4.75M/176M [00:05<01:54, 1.49MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00077-of-00082.safetensors:  13% 23.4M/176M [00:11<00:20, 7.45MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:   4% 7.15M/176M [00:05<01:06, 2.54MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:   6% 11.2M/176M [00:06<00:33, 4.94MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00075-of-00082.safetensors:  63% 110M/176M [00:17<00:07, 8.23MB/s] \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:   8% 14.3M/176M [00:09<01:20, 2.00MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00078-of-00082.safetensors:   5% 7.58M/142M [00:09<02:50, 787kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00074-of-00082.safetensors:  14% 19.9M/142M [00:23<00:56, 2.16MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:   9% 16.3M/176M [00:15<03:06, 855kB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00077-of-00082.safetensors:  19% 33.9M/176M [00:20<01:28, 1.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00074-of-00082.safetensors:  61% 87.0M/142M [00:28<00:17, 3.22MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00074-of-00082.safetensors: 100% 142M/142M [00:28<00:00, 4.95MB/s]\n",
      "Fetching 82 files:  90% 74/82 [04:33<00:42,  5.34s/it]\n",
      "\n",
      "\n",
      "\n",
      "model-00082-of-00082.safetensors:   0% 0.00/70.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00075-of-00082.safetensors: 100% 176M/176M [00:28<00:00, 6.26MB/s]\n",
      "Fetching 82 files:  91% 75/82 [04:34<00:29,  4.22s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00078-of-00082.safetensors:  53% 74.7M/142M [00:20<00:16, 4.06MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00076-of-00082.safetensors:  60% 85.4M/142M [00:27<00:16, 3.51MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:  10% 17.8M/176M [00:20<04:18, 610kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00077-of-00082.safetensors:  54% 95.0M/176M [00:30<00:19, 4.18MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:  49% 85.8M/176M [00:25<00:16, 5.53MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00077-of-00082.safetensors: 100% 176M/176M [00:31<00:00, 5.59MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00078-of-00082.safetensors: 100% 142M/142M [00:26<00:00, 5.28MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model-00076-of-00082.safetensors: 100% 142M/142M [00:33<00:00, 4.19MB/s]\n",
      "Fetching 82 files:  93% 76/82 [04:40<00:28,  4.82s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00080-of-00082.safetensors:   0% 8.13k/142M [00:28<139:10:05, 283B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00081-of-00082.safetensors:  19% 60.9M/328M [00:24<01:47, 2.48MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00081-of-00082.safetensors:  39% 128M/328M [00:25<00:32, 6.07MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00081-of-00082.safetensors:  60% 195M/328M [00:25<00:11, 11.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:  83% 146M/176M [00:31<00:04, 7.30MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00081-of-00082.safetensors:  80% 261M/328M [00:26<00:03, 17.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00081-of-00082.safetensors: 100% 328M/328M [00:30<00:00, 17.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00081-of-00082.safetensors: 100% 328M/328M [00:30<00:00, 10.9MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:  84% 147M/176M [00:35<00:05, 5.35MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00082-of-00082.safetensors:   1% 472k/70.8M [00:15<39:28, 29.7kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00080-of-00082.safetensors:   2% 3.32M/142M [00:34<13:25, 172kB/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:  87% 153M/176M [00:35<00:03, 5.80MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00082-of-00082.safetensors:   5% 3.58M/70.8M [00:15<03:39, 307kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00080-of-00082.safetensors:   3% 4.86M/142M [00:34<07:51, 290kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:  89% 157M/176M [00:36<00:02, 6.30MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00082-of-00082.safetensors:  10% 6.93M/70.8M [00:16<01:28, 723kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00080-of-00082.safetensors:   5% 6.43M/142M [00:34<04:55, 458kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00082-of-00082.safetensors:  17% 12.2M/70.8M [00:16<00:35, 1.63MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:  91% 160M/176M [00:36<00:02, 6.72MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00082-of-00082.safetensors:  23% 16.3M/70.8M [00:16<00:21, 2.59MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:  93% 163M/176M [00:36<00:01, 7.23MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00080-of-00082.safetensors:   6% 8.03M/142M [00:35<03:13, 691kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:  95% 166M/176M [00:36<00:01, 8.23MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00080-of-00082.safetensors:   7% 9.46M/142M [00:35<02:14, 981kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00082-of-00082.safetensors:  91% 64.1M/70.8M [00:16<00:00, 16.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00080-of-00082.safetensors:   8% 10.8M/142M [00:35<01:51, 1.18MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00080-of-00082.safetensors:  49% 69.4M/142M [00:38<00:06, 10.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00079-of-00082.safetensors:  96% 169M/176M [00:39<00:01, 3.44MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00082-of-00082.safetensors: 100% 70.8M/70.8M [00:20<00:00, 3.54MB/s]\n",
      "model-00079-of-00082.safetensors: 100% 176M/176M [00:40<00:00, 4.39MB/s]\n",
      "Fetching 82 files:  96% 79/82 [04:54<00:13,  4.61s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00080-of-00082.safetensors:  51% 71.6M/142M [00:38<00:06, 11.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00080-of-00082.safetensors:  53% 74.6M/142M [00:39<00:07, 8.71MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model-00080-of-00082.safetensors: 100% 142M/142M [00:40<00:00, 3.53MB/s]\n",
      "Fetching 82 files: 100% 82/82 [04:55<00:00,  3.60s/it]\n",
      "Loading checkpoint shards: 100% 82/82 [01:08<00:00,  1.20it/s]\n",
      "generation_config.json: 100% 183/183 [00:00<00:00, 1.34MB/s]\n",
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "tokenizer_config.json: 100% 902/902 [00:00<00:00, 6.16MB/s]\n",
      "tokenizer.model: 100% 500k/500k [00:00<00:00, 875kB/s]\n",
      "tokenizer.json: 100% 3.62M/3.62M [00:00<00:00, 15.7MB/s]\n",
      "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.99MB/s]\n",
      "> Changing padding side\n",
      "q=02_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q2\n",
      "q=19_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q19\n",
      "q=21_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q21\n",
      "q=42_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q42\n",
      "q=62_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q62\n",
      "q=63_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q63\n",
      "q=77_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q77\n",
      "q=78_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q78\n",
      "q=83_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q83\n",
      "q=84_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q84\n",
      "q=87_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q87\n",
      "q=88_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q88\n",
      "q=124_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "Skipping Q124\n",
      "q=126_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "> Trimming Dataset from 57\n",
      "> Prompting meta-llama/Llama-2-13b-chat-hf with Q126\n",
      "100% 218/218 [34:51<00:00,  9.59s/it]\n",
      "q=127_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "> Prompting meta-llama/Llama-2-13b-chat-hf with Q127\n",
      "100% 275/275 [44:15<00:00,  9.65s/it]\n",
      "q=142_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "> Prompting meta-llama/Llama-2-13b-chat-hf with Q142\n",
      "100% 275/275 [43:55<00:00,  9.58s/it]\n",
      "q=143_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "> Prompting meta-llama/Llama-2-13b-chat-hf with Q143\n",
      "100% 275/275 [43:30<00:00,  9.49s/it]\n",
      "q=149_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "> Prompting meta-llama/Llama-2-13b-chat-hf with Q149\n",
      "100% 275/275 [42:30<00:00,  9.28s/it]\n",
      "q=150_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "> Prompting meta-llama/Llama-2-13b-chat-hf with Q150\n",
      "100% 275/275 [42:22<00:00,  9.25s/it]\n",
      "q=171_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0\n",
      "> Prompting meta-llama/Llama-2-13b-chat-hf with Q171\n",
      " 35% 96/275 [15:12<28:38,  9.60s/it]"
     ]
    }
   ],
   "source": [
    "!python wvs_query_hf.py \\\n",
    "    --qid -1 \\\n",
    "    --model meta-llama/Llama-2-13b-chat-hf \\\n",
    "    --lang en \\\n",
    "    --country us \\\n",
    "    --max-tokens 4 --temperature 0.7 --batch-size 1 --n-gen 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMhaPjRWT1h6"
   },
   "outputs": [],
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2rwCdxsTrcx",
    "outputId": "8d93318b-e9e2-4bf9-b197-7719cc048ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 Personas\n",
      "Language=en | Temperature=0.7 | Tokens=16 | N=5 | Batch=1 | Version=1\n",
      "> Device cuda:0\n",
      "> Running 1 Qs\n",
      "2025-08-06 22:43:42.960120: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754520222.987048   32783 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754520222.995407   32783 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754520223.029148   32783 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754520223.029180   32783 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754520223.029184   32783 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754520223.029189   32783 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-06 22:43:43.034174: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100% 82/82 [01:20<00:00,  1.01it/s]\n",
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "> Changing padding side\n",
      "q=02_lang=en_country=us_temp=0.7_maxt=16_n=5_v1_fewshot=0\n",
      "> Prompting meta-llama/Llama-2-13b-chat-hf with Q2\n",
      "100% 275/275 [2:45:39<00:00, 36.14s/it]\n"
     ]
    }
   ],
   "source": [
    "!python wvs_query_hf.py \\\n",
    "    --qid 2 \\\n",
    "    --model meta-llama/Llama-2-13b-chat-hf \\\n",
    "    --lang en \\\n",
    "    --country us \\\n",
    "    --max-tokens 16 --temperature 0.7 --batch-size 1 --n-gen 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSyUGVkBhTL2"
   },
   "outputs": [],
   "source": [
    "# We change prompt and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQ3IcQnqhS_H",
    "outputId": "b6647f95-c1aa-41da-eb32-c4ab6033bc6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 Personas\n",
      "Language=en | Temperature=0.1 | Tokens=4 | N=1 | Batch=1 | Version=1\n",
      "> Device cuda:0\n",
      "> Running 1 Qs\n",
      "2025-08-13 04:01:14.188990: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755057674.336989    9151 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755057674.345977    9151 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755057674.379420    9151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755057674.379456    9151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755057674.379461    9151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755057674.379465    9151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-13 04:01:14.384277: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100% 82/82 [00:59<00:00,  1.38it/s]\n",
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "> Changing padding side\n",
      "q=19_lang=en_country=us_temp=0.1_maxt=4_n=1_v1_fewshot=0\n",
      "> Prompting meta-llama/Llama-2-13b-chat-hf with Q19\n",
      "100% 275/275 [41:58<00:00,  9.16s/it]\n"
     ]
    }
   ],
   "source": [
    "!python wvs_query_hf.py \\\n",
    "  --qid 19 \\\n",
    "  --model meta-llama/Llama-2-13b-chat-hf \\\n",
    "  --lang en \\\n",
    "  --country us \\\n",
    "  --max-tokens 4 --temperature 0.1 --batch-size 1 --n-gen 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTS-j4yJYriQ"
   },
   "outputs": [],
   "source": [
    "# Update temperature and sampling settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4g4L1I7VY2sW"
   },
   "outputs": [],
   "source": [
    "!python wvs_query_hf.py \\\n",
    "    --qid 19 \\\n",
    "    --model meta-llama/Llama-2-13b-chat-hf \\\n",
    "    --lang en \\\n",
    "    --country us \\\n",
    "    --max-tokens 4 --temperature 0.0 --batch-size 1 --n-gen 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4SKXachXYsz"
   },
   "source": [
    "#### Inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edy3XP-06p2b",
    "outputId": "7ca218d3-8d81-4f40-8cd9-db53d1d55d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/drive/MyDrive/cultural-trends/scripts/wvs_query_hf.py\n"
     ]
    }
   ],
   "source": [
    "# %load /content/drive/MyDrive/cultural-trends/scripts/wvs_query_hf.py\n",
    "%%writefile /content/drive/MyDrive/cultural-trends/scripts/wvs_query_hf.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, AutoConfig\n",
    "\n",
    "from wvs_dataset import WVSDataset\n",
    "from utils import read_json, write_json\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity(50)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "OFFLOAD_DIR = \"/content/offload\"\n",
    "os.makedirs(OFFLOAD_DIR, exist_ok=True)\n",
    "\n",
    "#bnb_config = BitsAndBytesConfig(\n",
    "#   load_in_8bit=True,\n",
    "#    llm_int8_enable_fp32_cpu_offload=True\n",
    "#)\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, fewshot_cache, prompts, device, n_steps=20):\n",
    "    # generation cycle with 20 steps\n",
    "    step = 0\n",
    "    past_key_values = fewshot_cache\n",
    "    tokens = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    output = None\n",
    "    while step < n_steps:\n",
    "        attention_mask = input_ids.new_ones(input_ids.shape)\n",
    "\n",
    "        if output is not None:\n",
    "            past_key_values = output[\"past_key_values\"]\n",
    "\n",
    "        ids = model.prepare_inputs_for_generation(input_ids,\n",
    "                                                past=past_key_values,\n",
    "                                                attention_mask=attention_mask,\n",
    "                                                use_cache=True)\n",
    "\n",
    "        output = model(**ids)\n",
    "\n",
    "        # next_token = random.choice(torch.topk(output.logits[:, -1, :], top_k, dim=-1).indices[0])\n",
    "        next_token = output.logits[:, -1, :].argmax(dim=-1)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def query_hf(\n",
    "    qid: str,\n",
    "    *,\n",
    "    model_name: str = 'bigscience/mt0-small',\n",
    "    version: int = 1,\n",
    "    lang: str = 'en',\n",
    "    max_tokens: int = 8,\n",
    "    temperature: float = 0.7,\n",
    "    n_gen: int = 5,\n",
    "    batch_size: int = 4,\n",
    "    fewshot: int = 0,\n",
    "    cuda: int = 0,\n",
    "    greedy: bool = False,\n",
    "    generator = None,\n",
    "    tokenizer = None,\n",
    "    no_persona = False,\n",
    "    subset = None,\n",
    "    country: str = \"egypt\",\n",
    "):\n",
    "\n",
    "    model_name_ = model_name.split(\"/\")[-1]\n",
    "    savedir = f\"../results_wvs_2/{model_name_}/{lang}\"\n",
    "    if not os.path.isdir(savedir):\n",
    "        os.makedirs(savedir)\n",
    "\n",
    "    filepath = f\"../dataset/wvs_template.{lang}.yml\"\n",
    "\n",
    "    dataset = WVSDataset(filepath,\n",
    "        language=lang,\n",
    "        country=country,\n",
    "        api=False,\n",
    "        model_name=model_name_,\n",
    "        use_anthro_prompt=False\n",
    "    )\n",
    "\n",
    "    device = torch.device(f'cuda:{cuda}' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Language={lang} | Temperature={temperature} | Tokens={max_tokens} | N={n_gen} | Batch={batch_size} | Version={version}\")\n",
    "    print(f\"> Device {device}\")\n",
    "\n",
    "    if qid <= 0:\n",
    "        question_ids = dataset.question_ids\n",
    "    else:\n",
    "        question_ids = [f\"Q{qid}\"]\n",
    "\n",
    "    print(f\"> Running {len(question_ids)} Qs\")\n",
    "    model_path = model_name\n",
    "    if \"mt0\" in model_name_:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"cpu\", torch_dtype=torch.float16).to(device)\n",
    "    else:\n",
    "        # model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, trust_remote_code=True).to(device)\n",
    "        if \"AceGPT\" in model_name:\n",
    "            model_path = \"/mnt/u14157_ic_nlp_001_files_nfs/nlpdata1/home/bkhmsi/models/models--FreedomIntelligence--AceGPT-13B-chat/snapshots/ab87ccbc2c4a05969957755aaabc04400bb20052\"\n",
    "        elif \"Llama\" in model_name:\n",
    "            model_path = \"eri00eli/llama2-13b-8bit\"\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                #quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                low_cpu_mem_usage=True,\n",
    "                #offload_folder=OFFLOAD_DIR,\n",
    "                trust_remote_code=True,\n",
    "                #local_files_only=True,\n",
    "            )\n",
    "            #model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       # model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"cpu\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                              trust_remote_code=True\n",
    "                                              #local_files_only=True\n",
    "                                              )\n",
    "\n",
    "    if\"Llama-2-13b-chat-hf\" in model_name or \"AceGPT-13B-chat\" in model_name:\n",
    "        print(\"> Changing padding side\")\n",
    "        tokenizer.padding_side = \"left\"\n",
    "\n",
    "    if model_name == \"gpt2\" or \"Sheared-LLaMA-1.3B\" in model_name or \"Llama-2-13b\" in model_name or \"AceGPT-13B-chat\" in model_name:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    for qid in question_ids:\n",
    "        qid = int(qid[1:])\n",
    "        dataset.set_question(index=qid)\n",
    "\n",
    "        filesuffix = f\"q={str(qid).zfill(2)}_lang={lang}_country={country}_temp={temperature}_maxt={max_tokens}_n={n_gen}_v{version}_fewshot={fewshot}\"\n",
    "        print(filesuffix)\n",
    "\n",
    "        preds_path = os.path.join(savedir, f\"preds_{filesuffix}.json\")\n",
    "\n",
    "        completions = []\n",
    "        if os.path.exists(preds_path):\n",
    "            completions = read_json(preds_path)\n",
    "\n",
    "        if len(completions) >= len(dataset):\n",
    "            print(f\"Skipping Q{qid}\")\n",
    "            continue\n",
    "\n",
    "        if len(completions) > 0:\n",
    "            print(f\"> Trimming Dataset from {len(completions)}\")\n",
    "            dataset.trim_dataset(len(completions))\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=2, shuffle=False)\n",
    "\n",
    "        if fewshot > 0:\n",
    "\n",
    "            fewshot_examples, _ = dataset.fewshot_examples()\n",
    "            fewshot_tokens = tokenizer(fewshot_examples, padding=True, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                fewshot_cache = model(**fewshot_tokens, use_cache=True)[\"past_key_values\"]\n",
    "\n",
    "        index = 0\n",
    "        print(f\"> Prompting {model_name} with Q{qid}\")\n",
    "        for batch_idx, prompts in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "            if fewshot == 0:\n",
    "                tokens = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "                gen_outputs = model.generate(**tokens,\n",
    "                    temperature=0.0,\n",
    "                    do_sample=False,\n",
    "                    num_return_sequences=n_gen,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                )\n",
    "                decoded_output = tokenizer.batch_decode(gen_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "                for b_i in range(0, len(decoded_output), n_gen):\n",
    "                    preds = decoded_output[b_i:b_i+n_gen]\n",
    "                    preds = [pred.replace(prompts[b_i//n_gen], \"\") for pred in preds]\n",
    "                    persona = dataset.persona_qid[f\"Q{qid}\"][index]\n",
    "                    q_info = dataset.question_info[f\"Q{qid}\"][index]\n",
    "                    index += 1\n",
    "                    completions += [{\n",
    "                        \"persona\": persona,\n",
    "                        \"question\": q_info,\n",
    "                        \"response\": preds,\n",
    "                    }]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # prompts_with_fewshot = [fewshot_examples + prompt for prompt in prompts]\n",
    "                # tokens_with_fewshot = tokenizer(prompts_with_fewshot, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "                # start_time = time.time()\n",
    "                # gen_outputs_wo_cache = model.generate(**tokens_with_fewshot,\n",
    "                #     temperature=temperature,\n",
    "                #     do_sample=(not greedy),\n",
    "                #     num_return_sequences=n_gen,\n",
    "                #     max_new_tokens=max_tokens,\n",
    "                # )\n",
    "\n",
    "                # decoded_output = tokenizer.batch_decode(gen_outputs_wo_cache, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "                # tokens = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "                # tokens[\"input_ids\"] = tokens[\"input_ids\"][:, 1:]\n",
    "                # tokens[\"attention_mask\"] = tokens[\"attention_mask\"][:, 1:]\n",
    "\n",
    "                # with torch.no_grad():\n",
    "                #     prompt_cache = model(**tokens_with_fewshot, past_key_values=fewshot_cache, use_cache=True)[\"past_key_values\"]\n",
    "\n",
    "                # tokens_with_fewshot_concat = {\n",
    "                #     \"input_ids\": torch.cat([fewshot_tokens[\"input_ids\"].repeat(batch_size, 1), tokens[\"input_ids\"][:, 1:]], dim=1),\n",
    "                #     \"attention_mask\": torch.cat([fewshot_tokens[\"attention_mask\"].repeat(batch_size, 1), tokens[\"attention_mask\"][:, 1:]],dim=1),\n",
    "                #     # \"attention_mask\": torch.cat([fewshot_tokens[\"attention_mask\"].repeat(batch_size, 1), tokens[\"attention_mask\"][:, 1:], torch.ones(batch_size, 1).to(device)],dim=1),\n",
    "                # }\n",
    "\n",
    "                # tokens_with_fewshot_concat = {\n",
    "                #     \"input_ids\": torch.cat([fewshot_tokens[\"input_ids\"].repeat(batch_size, 1), tokens[\"input_ids\"]], dim=1),\n",
    "                #     \"attention_mask\": torch.cat([fewshot_tokens[\"attention_mask\"].repeat(batch_size, 1), tokens[\"attention_mask\"]],dim=1),\n",
    "                #     # \"attention_mask\": torch.cat([fewshot_tokens[\"attention_mask\"].repeat(batch_size, 1), tokens[\"attention_mask\"], torch.zeros(batch_size, 1).to(device)],dim=1),\n",
    "                # }\n",
    "\n",
    "                # num_layers = len(fewshot_cache)\n",
    "                # all_cache = []\n",
    "                # for layer_idx in range(num_layers):\n",
    "                #     all_cache += [(\n",
    "                #         torch.cat([fewshot_cache[layer_idx][0].repeat(batch_size*n_gen,1,1,1), prompt_cache[layer_idx][0].repeat(n_gen,1,1,1)[:,:,:-1,:]], dim=2),\n",
    "                #         torch.cat([fewshot_cache[layer_idx][1].repeat(batch_size*n_gen,1,1,1), prompt_cache[layer_idx][1].repeat(n_gen,1,1,1)[:,:,:-1,:]], dim=2),\n",
    "                #     )]\n",
    "                    # all_cache += [(\n",
    "                    #     torch.cat([fewshot_cache[layer_idx][0].repeat(batch_size*n_gen,1,1,1), prompt_cache[layer_idx][0].repeat(n_gen,1,1,1)], dim=2),\n",
    "                    #     torch.cat([fewshot_cache[layer_idx][1].repeat(batch_size*n_gen,1,1,1), prompt_cache[layer_idx][1].repeat(n_gen,1,1,1)], dim=2),\n",
    "                    # )]\n",
    "\n",
    "                # print(\"Fewshot Tokens + Prompt Tokens: \", fewshot_tokens[\"input_ids\"].size(1) + tokens[\"input_ids\"].size(1))\n",
    "                # print(\"[Fewshot, Prompt] Tokens: \", tokens_with_fewshot_concat[\"input_ids\"].size(1))\n",
    "                # # print(\"(Fewshot + Prompt) Tokens: \", tokens_with_fewshot[\"input_ids\"].size(1))\n",
    "                # print(\"Cache Concat: \", all_cache[0][0].size())\n",
    "                # print(\"[Fewshot, Prompt] Attention Mask: \", tokens_with_fewshot_concat[\"attention_mask\"].size(1))\n",
    "                # breakpoint()\n",
    "                # # del prompt_cache\n",
    "\n",
    "                # tokens_with_fewshot[\"attention_mask\"] = torch.cat([tokens_with_fewshot[\"attention_mask\"], torch.ones(batch_size,1).to(device)], dim=1)\n",
    "\n",
    "                # # start_time = time.time()\n",
    "                # gen_outputs = model.generate(**tokens_with_fewshot_concat,\n",
    "                #     # input_ids=tokens_with_fewshot_concat[\"input_ids\"],\n",
    "                #     temperature=temperature,\n",
    "                #     do_sample=(not greedy),\n",
    "                #     num_return_sequences=n_gen,\n",
    "                #     max_new_tokens=max_tokens,\n",
    "                #     past_key_values=tuple(all_cache)\n",
    "                # )\n",
    "\n",
    "                gen_outputs = generate(model, tokenizer, fewshot_cache, prompts, device, n_steps=max_tokens)\n",
    "\n",
    "                # gen_outputs = model.generate(**tokens_with_fewshot_concat,\n",
    "                #     temperature=temperature,\n",
    "                #     do_sample=(not greedy),\n",
    "                #     num_return_sequences=n_gen,\n",
    "                #     max_new_tokens=max_tokens,\n",
    "                #     past_key_values=tuple(all_cache)\n",
    "                # )\n",
    "\n",
    "                decoded_output = tokenizer.batch_decode(gen_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "                for b_i in range(0, len(decoded_output), n_gen):\n",
    "                    preds = decoded_output[b_i:b_i+n_gen]\n",
    "                    preds = [pred.replace(fewshot_examples, \"\").replace(prompts[b_i//n_gen], \"\") for pred in preds]\n",
    "                    persona = dataset.persona_qid[f\"Q{qid}\"][index]\n",
    "                    q_info = dataset.question_info[f\"Q{qid}\"][index]\n",
    "                    index += 1\n",
    "                    completions += [{\n",
    "                        \"persona\": persona,\n",
    "                        \"question\": q_info,\n",
    "                        \"response\": preds,\n",
    "                    }]\n",
    "\n",
    "            write_json(preds_path, completions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # python wvs_query_hf_generate.py --qid -1 --model FreedomIntelligence/AceGPT-13B-chat --lang en --fewshot 3 --cuda 0\n",
    "    # python wvs_query_hf_generate.py --qid -1 --model princeton-nlp/Sheared-LLaMA-1.3B --max-tokens 10 --lang en --fewshot 3 --cuda 0\n",
    "    # python wvs_query_hf_generate.py --qid -1 --model princeton-nlp/Sheared-LLaMA-1.3B --max-tokens 5 --lang ar --fewshot 3 --cuda 0 --n-gen 1\n",
    "    # python wvs_query_hf_generate.py --qid -1 --model meta-llama/Llama-2-13b-chat-hf --max-tokens 16 --lang en --fewshot 0 --cuda 0 --n-gen 5 --batch-size 4\n",
    "\n",
    "    # python wvs_query_hf_generate.py --qid -1 --model meta-llama/Llama-2-13b-chat-hf --max-tokens 32 --lang ar --fewshot 0 --cuda 1 --n-gen 5 --batch-size 4\n",
    "    # python wvs_query_hf_generate.py --qid -1 --model FreedomIntelligence/AceGPT-13B-chat --max-tokens 5 --lang ar --fewshot 0 --cuda 0 --n-gen 5 --batch-size 4\n",
    "    # python wvs_query_hf_generate.py --qid -1 --model FreedomIntelligence/AceGPT-13B-chat --max-tokens 5 --lang en --fewshot 0 --cuda 0 --n-gen 5 --batch-size 4\n",
    "    # python wvs_query_hf_generate.py --qid -1 --model bigscience/mt0-xxl --max-tokens 5 --lang en --fewshot 0 --cuda 1 --n-gen 5 --batch-size 4 --country us\n",
    "\n",
    "    # cp -r /home/bkhmsi/.cache/huggingface/hub/models--FreedomIntelligence--AceGPT-13B-chat /mnt/u14157_ic_nlp_001_files_nfs/nlpdata1/home/bkhmsi/models\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--qid', required=True, type=int, help='question index')\n",
    "    parser.add_argument('--model', default=\"bigscience/mt0-small\", help='model to use')\n",
    "    parser.add_argument('--version', default=1, help='dataset version number')\n",
    "    parser.add_argument('--lang', default=\"en\", help='language')\n",
    "    parser.add_argument('--max-tokens', default=4, type=int, help='maximum number of output tokens')\n",
    "    parser.add_argument('--temperature', default=0.7, type=float, help='temperature')\n",
    "    parser.add_argument('--n-gen', default=5, type=int, help='number of generations')\n",
    "    parser.add_argument('--batch-size', default=4, type=int, help='batch size')\n",
    "    parser.add_argument('--fewshot', default=0, type=int, help='fewshot examples')\n",
    "    parser.add_argument('--cuda', default=0, type=int, help='cuda device number')\n",
    "    parser.add_argument('--greedy', action=\"store_true\", help='greedy decoding')\n",
    "    parser.add_argument('--country', type=str, help='country')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.greedy:\n",
    "        args.n_gen = 1\n",
    "        args.temperature = 1.0\n",
    "\n",
    "    qid = int(args.qid)\n",
    "\n",
    "    query_hf(\n",
    "        qid=qid,\n",
    "        model_name=args.model,\n",
    "        version=args.version,\n",
    "        lang=args.lang,\n",
    "        max_tokens=args.max_tokens,\n",
    "        temperature=float(args.temperature),\n",
    "        n_gen=int(args.n_gen),\n",
    "        batch_size=int(args.batch_size),\n",
    "        fewshot=int(args.fewshot),\n",
    "        cuda=args.cuda,\n",
    "        greedy=args.greedy,\n",
    "        country=args.country,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vQep8T9DSXA"
   },
   "source": [
    "### Majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpc00vmuama3",
    "outputId": "bbb79ba9-5e77-4cbf-f12b-31fd62a93688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Llama-2-13b-chat-hf #########\n",
      "\n",
      " Reading survey data from: ../dataset/us_wvs_wave7_v7_n303.csv\n",
      " Survey shape: (303, 395)\n",
      "['Unnamed: 0', 'version Version of Data File', 'doi Digital Object Identifier', 'A_YEAR Year of survey', 'B_COUNTRY ISO 3166-1 numeric country code', 'B_COUNTRY_ALPHA ISO 3166-1 alpha-3 country code', 'C_COW_NUM CoW country code numeric', 'C_COW_ALPHA CoW country code alpha', 'D_INTERVIEW Interview ID', 'J_INTDATE Date of interview', 'FW_START Year/month of start-fieldwork', 'FW_END Year/month of end-fieldwork', 'K_TIME_START Start time of the interview [HH.MM]', 'K_TIME_END End time of the interview [HH.MM]', 'K_DURATION Total length of interview [minutes]', 'Q_MODE Mode of data collection', 'N_REGION_ISO Region ISO 3166-2', 'N_REGION_WVS Region country specific', 'N_TOWN Settlement name', 'G_TOWNSIZE Settlement size_8 groups', 'G_TOWNSIZE2 Settlement size_5 groups', 'H_SETTLEMENT Settlement type', 'H_URBRURAL Urban-Rural', 'O1_LONGITUDE Geographical Coordinates - Longitude', 'O2_LATITUDE Geographical Coordinates - Latitude', 'S_INTLANGUAGE Language in which interview was conducted', 'LNGE_ISO Language in which interview was conducted  (ISO 639-1 Alpha 2 digit)', 'E_RESPINT Respondent interested during the interview', 'F_INTPRIVACY Interview privacy', 'W_WEIGHT Weight', 'S018 Equilibration weight-1000', 'PWGHT Population size weight', 'Q1 Important in life: Family', 'Q2 Important in life: Friends', 'Q3 Important in life: Leisure time', 'Q4 Important in life: Politics', 'Q5 Important in life: Work', 'Q6 Important in life: Religion', 'Q7 Important child qualities: good manners', 'Q8 Important child qualities: independence', 'Q9 Important child qualities: hard work', 'Q10 Important child qualities: feeling of responsibility', 'Q11 Important child qualities: imagination', 'Q12 Important child qualities: tolerance and respect for other people', 'Q13 Important child qualities: thrift saving money and things', 'Q14 Important child qualities: determination perseverance', 'Q15 Important child qualities: religious faith', 'Q16 Important child qualities: unselfishness', 'Q17 Important child qualities: obedience', 'Q18 Neighbors: Drug addicts', 'Q19 Neighbors: People of a different race', 'Q20 Neighbors: People who have AIDS', 'Q21 Neighbors: Immigrants/foreign workers', 'Q22 Neighbors: Homosexuals', 'Q23 Neighbors: People of a different religion', 'Q24 Neighbors: Heavy drinkers', 'Q25 Neighbors: Unmarried couples living together', 'Q26 Neighbors: People who speak a different language', 'Q27 One of main goals in life has been to make my parents proud', 'Q28 Pre-school child suffers with working mother', 'Q29 Men make better political leaders than women do', 'Q30 University is more important for a boy than for a girl', 'Q31 Men make better business executives than women do', 'Q32 Being a housewife just as fulfilling', 'Q33 Jobs scarce: Men should have more right to a job than women', 'Q33_3 Jobs scarce: Men should have more right to a job than women (3-point scale)', 'Q34 Jobs scarce: Employers should give priority to (nation) people than immigrants', 'Q34_3 Jobs scarce: Employers should give priority to (nation) people than immigrants (3-point scale)', 'Q35 Problem if women have more income than husband', 'Q35_3 Problem if women have more income than husband (3-point scale)', 'Q36 Homosexual couples are as good parents as other couples', 'Q37 Duty towards society to have children', 'Q38 It is children duty to take care of ill parent', \"Q39 People who don't work turn lazy\", 'Q40 Work is a duty towards society', 'Q41 Work should  always come first even if it means less spare time', 'Q42 Basic kinds of attitudes concerning society', 'Q43 Future changes: Less importance placed on work', 'Q44 Future changes: More emphasis on technology', 'Q45 Future changes: Greater respect for authority', 'Q46 Feeling of happiness', 'Q47 State of health (subjective)', 'Q48 How much freedom of choice and control', 'Q49 Satisfaction with your life', 'Q50 Satisfaction with financial situation of household', 'Q51 Frequency you/family (last 12 month): Gone without enough food to eat', 'Q52 Frequency you/family (last 12 month): Felt unsafe from crime in your own home', 'Q53 Frequency you/family (last 12 month): Gone without needed medicine or treatment that you needed', 'Q54 Frequency you/family (last 12 month): Gone without a cash income', 'Q55 In the last 12 month, how often have you or your family: Gone without a safe shelter over your head', 'Q56 Standard of living comparing with your parents', 'Q57 Most people can be trusted', 'Q58 Trust: Your family', 'Q59 Trust: Your neighborhood', 'Q60 Trust: People you know personally', 'Q61 Trust: People you meet for the first time', 'Q62 Trust: People of another religion', 'Q63 Trust: People of another nationality', 'Q64 Confidence: Churches', 'Q65 Confidence: Armed Forces', 'Q66 Confidence: The Press', 'Q67 Confidence: Television', 'Q68 Confidence: Labor Unions', 'Q69 Confidence: The Police', 'Q70 Confidence: Justice System/Courts', 'Q71 Confidence: The Government', 'Q72 Confidence: The Political Parties', 'Q73 Confidence: Parliament', 'Q74 Confidence: The Civil Services', 'Q75 Confidence: Universities', 'Q76 Confidence: Elections', 'Q77 Confidence: Major Companies', 'Q78 Confidence: Banks', 'Q79 Confidence: The Environmental Protection Movement', \"Q80 Confidence: The Women's Movement\", 'Q81 Confidence: Charitable or humanitarian organizations', 'Q82 Confidence: Major regional organization (combined from country-specific)', 'Q82_NAFTA Confidence: The North American Free Trade Agreement (NAFTA)', 'Q83 Confidence: The United Nations (UN)', 'Q84 Confidence: International Monetary Found (IMF)', 'Q85 Confidence: International Criminal Court (ICC)', 'Q86 Confidence: North Atlantic Treaty Organization (NATO)', 'Q87 Confidence: The World Bank (WB)', 'Q88 Confidence: The World Health Organization (WHO)', 'Q89 Confidence: The World Trade Organization (WTO)', 'Q90 International organizations: being effective vs being democratic', 'Q91 Countries with the permanent seats on the UN Security Council', 'Q92 Where are the headquarters of the International Monetary Fund (IMF) located?', 'Q93 Which of the following problems does the organization Amnesty International deal with?', 'Q94 Active/Inactive membership: church or religious org', 'Q95 Active/Inactive membership: sport or recreational org', 'Q96 Active/Inactive membership: art, music, educational org', 'Q97 Active/Inactive membership: labor union', 'Q98 Active/Inactive membership: political party', 'Q99 Active/Inactive membership: environmental organization', 'Q100 Active/Inactive membership: professional organization', 'Q101 Active/Inactive membership: charitable/humanitarian organization', 'Q102 Active/Inactive membership: consumer organization', 'Q103 Active/Inactive membership: self-help group, mutual aid group', 'Q104 Active/Inactive membership: women’s group', 'Q105 Active/Inactive membership: other organization', 'Q106 Incomes should be made more equal vs There should be greater incentives for individual effort', 'Q107 Private vs state ownership of business', \"Q108 Government's vs individual's responsibility\", 'Q109 Competition good or harmful', 'Q110 Success: hard work vs luck', 'Q111 Protecting environment vs. Economic growth', 'Q112 Perceptions of corruption in the country', 'Q113 Involved in corruption: State authorities', 'Q114 Involved in corruption: Business executives', 'Q115 Involved in corruption: Local authorities', 'Q116 Involved in corruption: Civil service providers', 'Q117 Involved in corruption: Journalists and media', 'Q118 Frequency ordinary people pay a bribe, give a gift or do a favor to local officials/service providers in order to get services', 'Q119 Degree of agreement: On the whole, women are less corrupt than men', 'Q120 Risk to be held accountable for giving or receiving a bribe', 'Q121 Impact of immigrants on the development of the country', 'Q122 Immigration in your country: Fills useful jobs in the workforce', 'Q123 Immigration in your country: Strengthens cultural diversity', 'Q124 Immigration in your country: Increases the crime rate', 'Q125 Immigration in your country: Gives asylum to political refugees', 'Q126 Immigration in your country: Increases the risks of terrorism', 'Q127 Immigration in your country: Helps poor people establish new lives', 'Q128 Immigration in your country: Increases unemployment', 'Q129 Immigration in your country: Leads to social conflict', 'Q130 Immigration policy preference', 'Q131 Secure in neighborhood', 'Q132 Frequency in your neighborhood: Robberies', 'Q133 Frequency in your neighborhood: Alcohol consumed in the streets', 'Q134 Frequency in your neighborhood: Police or military interfere with people’s private life', 'Q135 Frequency in your neighborhood: Racist behavior', 'Q136 Frequency in your neighborhood: Drug sale in streets', 'Q137 Frequency in your neighborhood: Street violence and fights', 'Q138 Frequency in your neighborhood: Sexual harassment', 'Q139 Things done for reasons of security: Didn’t carry much money', 'Q140 Things done for reasons of security: Preferred not to go out at night', 'Q141 Things done for reasons of security: Carried a knife, gun or other weapon', 'Q142 Worries: Losing my job or not finding a job', \"Q143 Worries: Not being able to give one's children a good education\", 'Q144 Respondent was victim of a crime during the past year', \"Q145 Respondent's family was victim of a crime during last year\", 'Q146 Worries: A war involving my country', 'Q147 Worries: A terrorist attack', 'Q148 Worries: A civil war', 'Q149 Freedom and Equality - Which more important', 'Q150 Freedom and security - Which more important', 'Q151 Willingness to fight for country', 'Q152 Aims of country: first choice', 'Q153 Aims of country: second choice', 'Q154 Aims of respondent: first choice', 'Q155 Aims of respondent: second choice', 'Q156 Most important: first choice', 'Q157 Most important: second choice', 'Q158 Science and technology are making our lives healthier, easier, and more comfortable', 'Q159 Because of science and technology, there will be more opportunities for the next generation', 'Q160 We depend too much on science and not enough on faith', 'Q161 One of the bad effects of science is that it breaks down people’s ideas of right and wrong', 'Q162 It is not important for me to know about science in my daily life', 'Q163 The world is better off, or worse off, because of science and technology', 'Q164 Importance of God', 'Q165 Believe in: God', 'Q166 Believe in: life after death', 'Q167 Believe in: hell', 'Q168 Believe in: heaven', 'Q169 Whenever science and religion conflict,  religion is always right', 'Q170 The only acceptable religion  is my religion', 'Q171 How often do you attend religious services', 'Q172 How often do you pray', 'Q173 Religious person', 'Q174 Meaning of religion: To follow religious norms and ceremonies vs To do good to other people', 'Q175 Meaning of religion: To make sense of life after death vs To make sense of life in this world', 'Q176 Degree of agreement: Nowadays one often has trouble deciding which moral rules are the right ones to follow', 'Q177 Justifiable: Claiming government benefits to which you are not entitled', 'Q178 Justifiable: Avoiding a fare on public transport', 'Q179 Justifiable: Stealing property', 'Q180 Justifiable: Cheating on taxes', 'Q181 Justifiable: Someone accepting a bribe in the course of their duties', 'Q182 Justifiable: Homosexuality', 'Q183 Justifiable: Prostitution', 'Q184 Justifiable: Abortion', 'Q185 Justifiable: Divorce', 'Q186 Justifiable: Sex before marriage', 'Q187 Justifiable: Suicide', 'Q188 Justifiable: Euthanasia', 'Q189 Justifiable: For a man to beat his wife', 'Q190 Justifiable: Parents beating children', 'Q191 Justifiable: Violence against other people', 'Q192 Justifiable: Terrorism as a political, ideological or religious mean', 'Q193 Justifiable: Having casual sex', 'Q194 Justifiable: Political violence', 'Q195 Justifiable: Death penalty', 'Q196 Government has the right: Keep people under video surveillance in public areas', 'Q197 Government has the right: Monitor all e-mails and any other information exchanged on the Internet', 'Q198 Government has the right: Collect information about anyone living in [COUNTRY] without their knowledge', 'Q199 Interest in politics', 'Q200 How often discusses political matters with friends', 'Q201 Information source: Daily newspaper', 'Q202 Information source: TV news', 'Q203 Information source: Radio news', 'Q204 Information source: Mobile phone', 'Q205 Information source: Email', 'Q206 Information source: Internet', 'Q207 Information source: Social media (Facebook, Twitter, etc.)', 'Q208 Information source: Talk with friends or colleagues', 'Q209 Political action: Signing a petition', 'Q210 Political action: Joining in boycotts', 'Q211 Political action: Attending lawful/peaceful demonstrations', 'Q212 Political action: Joining unofficial strikes', 'Q213 Social activism: Donating to a group or campaign', 'Q214 Social activism: Contacting a government official', 'Q216 Social activism: Encouraging others to vote', 'Q217 Political actions online: Searching information about politics and political events', 'Q218 Political actions online: Signing an electronic petition', 'Q219 Political actions online: Encouraging other people to take any form of political action', 'Q220 Political actions online: Organizing political activities, events, protests', 'Q221 Vote in elections: local level', 'Q222 Vote in elections: national level', 'Q223 Which party would you vote for if there were a national election tomorrow', 'Q223_ABREV Party preference Abbreviation', 'Q223_LOCAL Party preference Local name', \"Q224 How often in country's elections: Votes are counted fairly\", \"Q225 How often in country's elections: Opposition candidates are prevented from running\", \"Q226 How often in country's elections: TV news favors the governing party\", \"Q227 How often in country's elections: Voters are bribed\", \"Q228 How often in country's elections: Journalists provide fair coverage of elections\", \"Q229 How often in country's elections: Election officials are fair\", \"Q230 How often in country's elections: Rich people buy elections\", \"Q231 How often in country's elections: Voters are threatened with  violence at the polls\", \"Q232 How often in country's elections: Voters are offered a genuine choice in the elections\", \"Q233 How often in country's elections: Women have equal opportunities to run the office\", 'Q234 Some people think that having honest elections makes a lot of difference in their lives;  other people think that it doesn’t matter much', 'Q235 Political system: Having a strong leader who does not have to bother with parliament and elections', 'Q236 Political system: Having experts, not government, make decisions according to what they think is best for the country', 'Q237 Political system: Having the army rule', 'Q238 Political system: Having a democratic political system', 'Q239 Political system: Having a system governed by religious law in which there are no political parties or elections', 'Q240 Left-right political scale', 'Q241 Democracy: Governments tax the rich and subsidize the poor', 'Q242 Democracy: Religious authorities interpret the laws', 'Q243 Democracy: People choose their leaders in free elections', 'Q244 Democracy: People receive state aid for unemployment', 'Q245 Democracy: The army takes over when government is incompetent', 'Q246 Democracy: Civil rights protect people’s liberty against oppression', \"Q247 Democracy: The state makes people's incomes equal\", 'Q248 Democracy: People obey their rulers', 'Q249 Democracy: Women have the same rights as men', 'Q250 Importance of democracy', 'Q251 How democratically is this country being governed today', 'Q252 Satisfaction with the political system performance', 'Q253 Respect for individual human rights nowadays', 'Q254 National pride', 'Q255 Feel close to your village, town or city', 'Q256 Feel close to your district, region', 'Q257 Feel close to your country', 'Q258 Feel close to your continent', 'Q259 Feel close to the world', 'Q260 Sex', 'Q261 Year of birth', 'Q262 Age', 'X003R Age recoded (6 intervals)', 'X003R2 Age recoded (3 intervals)', 'Q263 Respondent immigrant', 'Q264 Mother immigrant', 'Q265 Father immigrant', 'Q266 Country of birth: Respondent', 'Q267 Country of birth: Mother of the respondent', 'Q268 Country of birth: Father of the respondent', 'Q269 Respondent citizen', 'Q270 Number of people in household', 'Q271 Do you live with your parents', 'Q272 Language at home', 'Q273 Marital status', 'Q274 How many children do you have', 'Q275 Highest educational level: Respondent [ISCED 2011]', 'Q275A Highest educational level: Respondent (country specific)', 'Q275R Highest educational level: Respondent (recoded into 3 groups)', \"Q276 Highest educational level: Respondent's Spouse [ISCED 2011]\", \"Q276A Highest educational level: Respondent's Spouse (country specific)\", \"Q276R Highest educational level: Respondent's Spouse (recoded into 3 groups)\", \"Q277 Highest educational level: Respondent's Mother [ISCED 2011]\", \"Q277A Highest educational level: Respondent's Mother (country specific)\", \"Q277R Highest educational level: Respondent's Mother (recoded into 3 groups)\", \"Q278 Highest educational level: Respondent's Father [ISCED 2011]\", \"Q278A Highest educational level: Respondent's Father (country specific)\", \"Q278R Highest educational level: Respondent's Father (recoded into 3 groups)\", 'Q279 Employment status', \"Q280 Employment status - Respondent's Spouse\", 'Q281 Respondent - Occupational group', \"Q282 Respondent's Spouse - Occupational group\", \"Q283 Respondent's Father - Occupational group (when respondent was 14 years old)\", 'Q284 Sector of employment', 'Q285 Are you the chief wage earner in your house', 'Q286 Family savings during past year', 'Q287 Social class (subjective)', 'Q288 Scale of incomes', 'Q288R Income level (Recoded)', 'Q289 Religious denominations - major groups', 'Q289CS9 Religious denomination - detailed list', 'Q290 Ethnic group', 'Y001 Post-Materialist index 12-item', 'Y002 Post-Materialist index 4-item', 'Y003 Autonomy Index', 'SACSECVAL SACSECVAL.- Welzel Overall Secular Values', 'SACSECVALB Overall Secular Values Short version', 'RESEMAVAL RESEMAVAL.- Welzel emancipative values', 'RESEMAVALB Emancipative values Short version', 'I_AUTHORITY AUTHORITY - Welzel defiance - 1: Inverse respect for authority', 'I_NATIONALISM NATIONALISM - Welzel defiance - 2: Inverse national pride', 'I_DEVOUT DEVOUT- Welzel defiance - 3: Inverse devoutness', 'DEFIANCE DEFIANCE.- Welzel defiance sub-index', 'I_RELIGIMP RELIGIMP - Welzel disbelief- 1: Inverse importance of religion', 'I_RELIGBEL RELIGBEL - Welzel disbelief- 2: Inverse religious person', 'I_RELIGPRAC RELIGPRAC - Welzel disbelief- 3: Inverse religious practice', 'DISBELIEF DISBELIEF.- Welzel disbelief sub-index', 'I_NORM1 NORM1 - Welzel relativism- 1: Inverse norm conform1', 'I_NORM2 NORM2 - Welzel relativism- 2: Inverse norm conform2', 'I_NORM3 NORM3 - Welzel relativism- 3: Inverse norm conform3', 'RELATIVISM RELATIVISM.- Welzel relativism', 'I_TRUSTARMY TRUSTARMY- Welzel skepticism- 1: Inverse trust in army', 'I_TRUSTPOLICE TRUSTPOLICE- Welzel skepticism- 2: Inverse trust in police', 'I_TRUSTCOURTS TRUSTCOURTS- Welzel skepticism- 3: Inverse trust in courts', 'SCEPTICISM SCEPTICISM.- Welzel skepticism index', 'I_INDEP INDEP- Welzel autonomy-1: Independence as kid quality', 'I_IMAGIN IMAGIN- Welzel autonomy-2: Imagination as kid quality', 'I_NONOBED Emancipative Values-1: Obedience not kid quality', 'AUTONOMY AUTONOMY.- Wezel Autonomy sub index', 'I_WOMJOB WOMJOB- Welzel equality-1: Gender equality: job', 'I_WOMPOL WOMPOL- Welzel equality-2: Gender equality: politics', 'I_WOMEDU WOMEDU- Welzel equality-3: Gender equality: education', 'EQUALITY Emancipative Values-2: Equality sub-index', 'I_HOMOLIB HOMOLIB- Welzel choice-1: Homosexuality acceptance', 'I_ABORTLIB ABORTLIB- Welzel choice-2: Abortion acceptable', 'I_DIVORLIB DIVORLIB- Welzel choice-3: Divorce acceptable', 'CHOICE CHOICE.- Welzel choice sub-index', 'I_VOICE1 VOICE1- Welzel voice-1', 'I_VOICE2 VOICE2- Welzel voice-2', 'I_VOI2_00 VOI2_00- Welzel voice-3 (auxiliary)', 'VOICE VOICE.- Welzel voice sub-index', 'SECVALWGT Weight for overall secular values', 'WEIGHT1A Overall Secular Values-1: Weight1a', 'WEIGHT1B Emancipative Values-1: Weight1b', 'WEIGHT2A Overall Secular Values-2: Weight2a', 'WEIGHT2B Emancipative Values-2: Weight2b', 'WEIGHT3A Overall Secular Values-3: Weight', 'WEIGHT3B Emancipative Values-3: Weight3b', 'WEIGHT4A Overall Secular Values-4: Weight 4a', 'WEIGHT4B Emancipative Values-4: Weight 4b', 'RESEMAVALBWGT Weight for Emancipative values', 'RESEMAVALWGT Weight for Emancipative values', 'SECVALBWGT Weight for overall secular values Short Version', 'Y001_1 Materialist/postmaterialist 12-item index: Component 1', 'Y001_2 Materialist/postmaterialist 12-item index: Component 2', 'Y001_3 Materialist/postmaterialist 12-item index: Component 3', 'Y001_4 Materialist/postmaterialist 12-item index: Component 4', 'Y001_5 Materialist/postmaterialist 12-item index: Component 5']\n",
      " Mapped questions: {2: 'Q2 Important in life: Friends', 19: 'Q19 Neighbors: People of a different race', 21: 'Q21 Neighbors: Immigrants/foreign workers', 42: 'Q42 Basic kinds of attitudes concerning society', 62: 'Q62 Trust: People of another religion', 63: 'Q63 Trust: People of another nationality', 77: 'Q77 Confidence: Major Companies', 78: 'Q78 Confidence: Banks', 83: 'Q83 Confidence: The United Nations (UN)', 84: 'Q84 Confidence: International Monetary Found (IMF)', 87: 'Q87 Confidence: The World Bank (WB)', 88: 'Q88 Confidence: The World Health Organization (WHO)', 124: 'Q124 Immigration in your country: Increases the crime rate', 126: 'Q126 Immigration in your country: Increases the risks of terrorism', 127: 'Q127 Immigration in your country: Helps poor people establish new lives', 142: 'Q142 Worries: Losing my job or not finding a job', 143: \"Q143 Worries: Not being able to give one's children a good education\", 149: 'Q149 Freedom and Equality - Which more important', 150: 'Q150 Freedom and security - Which more important', 171: 'Q171 How often do you attend religious services', 175: 'Q175 Meaning of religion: To make sense of life after death vs To make sense of life in this world', 199: 'Q199 Interest in politics', 209: 'Q209 Political action: Signing a petition', 210: 'Q210 Political action: Joining in boycotts', 221: 'Q221 Vote in elections: local level', 224: \"Q224 How often in country's elections: Votes are counted fairly\", 229: \"Q229 How often in country's elections: Election officials are fair\", 234: 'Q234 Some people think that having honest elections makes a lot of difference in their lives;  other people think that it doesn’t matter much', 235: 'Q235 Political system: Having a strong leader who does not have to bother with parliament and elections', 236: 'Q236 Political system: Having experts, not government, make decisions according to what they think is best for the country', 239: 'Q239 Political system: Having a system governed by religious law in which there are no political parties or elections'} \n",
      "\n",
      "> Matching files: ['../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=02_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=124_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=126_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=127_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=142_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=143_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=149_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=150_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=171_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=175_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=199_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=19_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=209_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=210_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=21_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=221_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=224_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=229_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=234_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=235_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=236_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=239_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=42_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=62_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=63_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=77_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=78_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=83_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=84_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=87_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json', '../results_wvs_2/Llama-2-13b-chat-hf/en/preds_q=88_lang=en_country=us_temp=0.7_maxt=4_n=5_v1_fewshot=0.json']\n",
      "31  files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Options ['very important', 'rather important', 'not very important', 'not at all important', \"don't know\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 1/31 [00:00<00:24,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['agree', 'hard to say', 'disagree']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▋         | 2/31 [00:01<00:27,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['agree', 'hard to say', 'disagree']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|▉         | 3/31 [00:02<00:24,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['agree', 'hard to say', 'disagree']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 4/31 [00:03<00:22,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['very much', 'a good deal', 'not much', 'not at all']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 5/31 [00:04<00:21,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['very much', 'a good deal', 'not much', 'not at all']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 6/31 [00:05<00:20,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['freedom', 'equality']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 7/31 [00:05<00:19,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['freedom', 'security']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 8/31 [00:06<00:19,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['more than once a week', 'once a week', 'once a month', 'only on special holy days', 'once a year', 'less often', 'never, practically never']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 9/31 [00:07<00:18,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['to make sense of life after death', 'to make sense of life in this world']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 10/31 [00:08<00:17,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['very interested', 'somewhat interested', 'not very interested', 'not at all interested']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 11/31 [00:09<00:16,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['important', 'not mentioned', \"don't know\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▊      | 12/31 [00:19<01:09,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['have done', 'might do', 'would never do']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 13/31 [00:20<00:50,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['have done', 'might do', 'would never do']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 14/31 [00:20<00:37,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['important', 'not mentioned', \"don't know\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 15/31 [00:21<00:27,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['always', 'usually', 'never', 'not allowed to vote']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 16/31 [00:22<00:21,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['very often', 'fairly often', 'not often', 'not at all often']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▍    | 17/31 [00:23<00:17,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['very often', 'fairly often', 'not often', 'not at all often']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 18/31 [00:23<00:14,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['very important', 'rather important', 'not very important', 'not at all important']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████▏   | 19/31 [00:24<00:12,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['very good', 'fairly good', 'fairly bad', 'very bad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▍   | 20/31 [00:25<00:10,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['very good', 'fairly good', 'fairly bad', 'very bad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 21/31 [00:26<00:08,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['very good', 'fairly good', 'fairly bad', 'very bad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 22/31 [00:27<00:07,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['the entire way our society is organized must be radically changed by revolutionary action', 'our society must be gradually improved by reforms', 'our present society must be valiantly defended against all subversive forces', \"don't know\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 23/31 [00:27<00:06,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['trust completely', 'trust somewhat', 'do not trust very much', 'do not trust at all', \"don't know\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 24/31 [00:29<00:08,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['trust completely', 'trust somewhat', 'do not trust very much', 'do not trust at all', \"don't know\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████  | 25/31 [00:30<00:06,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['a great deal', 'quite a lot', 'not very much', 'none at all', \"don't know\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 26/31 [00:31<00:05,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['a great deal', 'quite a lot', 'not very much', 'none at all', \"don't know\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 27/31 [00:32<00:03,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['a great deal', 'quite a lot', 'not very much', 'none at all', \"don't know\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 28/31 [00:33<00:02,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['a great deal', 'quite a lot', 'not very much', 'none at all', \"don't know\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▎| 29/31 [00:34<00:01,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['a great deal', 'quite a lot', 'not very much', 'none at all', \"don't know\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 30/31 [00:34<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      " Options ['a great deal', 'quite a lot', 'not very much', 'none at all', \"don't know\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:35<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load raw responses 275\n",
      "Llama-2-13b-chat-hf | en: 2480/8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %load /content/drive/MyDrive/cultural-trends/scripts/wvs_majority_vote.py\n",
    "#%%writefile /content/drive/MyDrive/cultural-trends/scripts/wvs_majority_vote.py\n",
    "import os\n",
    "import re\n",
    "import  numpy as mp\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import (\n",
    "    read_json,\n",
    "    read_file,\n",
    "    read_yaml,\n",
    "    parse_range,\n",
    "    convert_to_dataframe,\n",
    "    create_wvs_question_map\n",
    ")\n",
    "\n",
    "# Column names in the survey CSV and human labels\n",
    "demographic_ids = [\n",
    "    \"N_REGION_WVS Region country specific\",\n",
    "    \"Q260 Sex\",\n",
    "    \"Q262 Age\",\n",
    "    \"Q266 Country of birth: Respondent\",\n",
    "    \"Q273 Marital status\",\n",
    "    \"Q275R Highest educational level: Respondent (recoded into 3 groups)\",\n",
    "    \"Q287 Social class (subjective)\"\n",
    "]\n",
    "\n",
    "demographic_txt = [\n",
    "    \"region\",\n",
    "    \"sex\",\n",
    "    \"age\",\n",
    "    \"country\",\n",
    "    \"marital_status\",\n",
    "    \"education\",\n",
    "    \"social_class\"\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # Config\n",
    "  COUNTRY = \"us\"\n",
    "  LANGS = [\"en\"]\n",
    "  MODELS = ['Llama-2-13b-chat-hf']\n",
    "  EVAL_METHOD = \"mv_sample\" # {\"flatten\", \"mv_sample\", \"mv_all\", \"first\"}\n",
    "  SCALE_QS    = False              # include scale-questions?\n",
    "\n",
    "  # Load selected questions\n",
    "  raw = read_file(\"../dataset/selected_questions.csv\")[0]\n",
    "\n",
    "  # Extract integers from \"Q1\", \"Q2\"\n",
    "  selected_questions = [int(q.strip()[1:]) for q in raw.split(\",\")]\n",
    "\n",
    "  # (unused) to collect any fully skipped Qs\n",
    "  invalid_question_indices = []\n",
    "\n",
    "  for LANG in LANGS:\n",
    "      for MODEL in MODELS:\n",
    "          print(f\"######### {MODEL} #########\\n\")\n",
    "\n",
    "          country_cap = \"US\" if COUNTRY == \"us\" else \"Egypt\"\n",
    "\n",
    "          # Build a map from Arabic labels → English if running Arabic prompts\n",
    "          demographic_map = {}\n",
    "          if LANG != \"en\":\n",
    "              print(\"> Building Demographic Map\")\n",
    "              ar_persona_parameters = read_yaml(\"../dataset/wvs_template.ar.yml\")[\"persona_parameters\"]\n",
    "              en_template_data      = read_yaml(\"../dataset/wvs_template.en.yml\")\n",
    "              for d_text in demographic_txt:\n",
    "                  if d_text == \"age\":\n",
    "                      continue  # numeric, no mapping needed\n",
    "                  # create capitalized label, e.g. \"Marital Status\"\n",
    "                  d_text_cap = ' '.join(map(str.capitalize, d_text.replace(\"_\",\" \").split()))\n",
    "                  # pick the right list of values for that demographic\n",
    "                  if d_text == \"region\":\n",
    "                      d_values = en_template_data[\"persona_parameters\"][d_text_cap][country_cap]\n",
    "                  else:\n",
    "                      d_values = en_template_data[\"persona_parameters\"][d_text_cap]\n",
    "\n",
    "                  demographic_map[d_text] = {}\n",
    "                  # zip Arabic list to English list, index-wise\n",
    "                  for idx, eng_val in enumerate(d_values):\n",
    "                      if d_text == \"region\":\n",
    "                          ar_val = ar_persona_parameters[d_text_cap][country_cap][idx]\n",
    "                      else:\n",
    "                          ar_val = ar_persona_parameters[d_text_cap][idx]\n",
    "                      demographic_map[d_text][ar_val] = eng_val\n",
    "\n",
    "          # Load and clean the ground truth WVS survey CSV for the chosen country\n",
    "          if COUNTRY == \"egypt\":\n",
    "            path =\"../dataset/eg_wvs_wave7_v7_n303.csv\"\n",
    "          else:\n",
    "            path =\"../dataset/us_wvs_wave7_v7_n303.csv\"\n",
    "          print(\" Reading survey data from:\", path)\n",
    "\n",
    "          # Load survey data using first row as header and semicolon as delimiter\n",
    "          survey_df = pd.read_csv(path, header=0, delimiter=\";\")\n",
    "          print(\" Survey shape:\", survey_df.shape)\n",
    "          print(list(survey_df.columns))\n",
    "\n",
    "          # Clean up the \"region\" column to remove prefixes like \"US:\"\n",
    "          region_col = demographic_ids[demographic_txt.index(\"region\")]\n",
    "          #print(region_col)\n",
    "          if COUNTRY == \"egypt\":\n",
    "            survey_df[region_col] = survey_df[region_col] = survey_df[region_col].str.replace(\"EG: \",\"\")\n",
    "          else:\n",
    "            survey_df[region_col] = survey_df[region_col].apply(lambda x: x.split(\":\")[-1][3:].strip())\n",
    "          #print(\"Region sample\", survey_df[region_col].unique()[:5])\n",
    "\n",
    "          # Map questions {2: 'Q2 Important in life', ..}\n",
    "          wvs_question_map = create_wvs_question_map(survey_df.columns.tolist(), selected_questions)\n",
    "          print(\" Mapped questions:\", wvs_question_map, \"\\n\")\n",
    "\n",
    "          # Load allowed answers\n",
    "          wvs_response_map = read_json(\"../dataset/wvs_response_map.json\")\n",
    "\n",
    "          # Determine where model outputs JSONs lives\n",
    "          if MODEL == \"gpt-3.5-turbo-1106\":\n",
    "            dirpath = f\"../results_wvs_2_gpt/{MODEL}/{LANG}\"\n",
    "          else:\n",
    "            dirpath = f\"../results_wvs_2/{MODEL}/{LANG}\"\n",
    "\n",
    "          options_dict = parse_range(read_json(\"../dataset/wvs_options.json\"))\n",
    "          wvs_themes = parse_range(read_json(\"../dataset/wvs_themes.json\"))\n",
    "          wvs_scale_questions = parse_range(read_json(\"../dataset/wvs_scale_questions.json\"))\n",
    "\n",
    "          wvs_questions = read_json(f\"../dataset/wvs_questions_dump.{LANG}.json\")\n",
    "\n",
    "          version_num = 3 if LANG == \"en\" and COUNTRY == \"egypt\" and MODEL in {\"gpt-3.5-turbo-0613\", \"mt0-xxl\"} else 1\n",
    "\n",
    "\n",
    "           # if COUNTRY == \"us\":\n",
    "            #     if \"Llama-2\" in MODEL:#å or \"AceGPT\" in MODEL:\n",
    "            #         filepaths = sorted(glob(os.path.join(dirpath, f\"*_country={COUNTRY}_*_maxt=32_n=5_v{version_num}_fewshot=0.json\")))\n",
    "            #     else:\n",
    "            #         filepaths = sorted(glob(os.path.join(dirpath, f\"*_country={COUNTRY}_*_v{version_num}*.json\")))\n",
    "            # else:\n",
    "            #     if \"Llama-2\" in MODEL or \"AceGPT\" in MODEL:\n",
    "            #         if LANG == \"en\" and 'Llama-2' in MODEL:\n",
    "            #             filepaths = sorted(glob(os.path.join(dirpath, f\"*maxt=32_n=5_v{version_num}_fewshot=0.json\")))\n",
    "            #         else:\n",
    "            #             filepaths = sorted(glob(os.path.join(dirpath, f\"*_v{version_num}_fewshot=0.json\")))\n",
    "            #     else:\n",
    "            #         filepaths = sorted(glob(os.path.join(dirpath, f\"*_v{version_num}.json\")))\n",
    "\n",
    "                # ar_filepaths = sorted(glob(os.path.join(f\"../results_wvs/{MODEL}/ar\", \"*_v1.json\")))\n",
    "\n",
    "\n",
    "          # Filtering outputs models files\n",
    "          if LANG == \"ar\" and MODEL == \"AceGPT-13B-chat\":\n",
    "            filepaths = sorted(glob(f\"{dirpath}/*_country={COUNTRY}_*_v2_*\"))\n",
    "          else:\n",
    "            filepaths = sorted(glob(f\"{dirpath}/*_country={COUNTRY}_*\"))\n",
    "\n",
    "          print(\"> Matching files:\", filepaths)\n",
    "          results = {demographic: [] for demographic in demographic_txt}\n",
    "          print(f\"{len(filepaths)}  files\")\n",
    "\n",
    "          all_invalids, all_num_responses  = 0, 0\n",
    "          for filepath in tqdm(filepaths):\n",
    "            if \"country = us\" in filepath and COUNTRY == \"egypt\":\n",
    "              continue\n",
    "\n",
    "            if \"country = egypt\" in filepath and COUNTRY == \"us\":\n",
    "              continue\n",
    "\n",
    "            if \"anthro = True\" in filepath:\n",
    "              continue\n",
    "\n",
    "            pattern = r'=(\\d+(\\.\\d+)?)'\n",
    "            matches = re.findall(pattern, filepath)\n",
    "            values = [match[0] for match in matches]\n",
    "            qidx = int(values[0])\n",
    "\n",
    "            save_dump_path = f\"../dumps_wvs_2/q={qidx}_country={COUNTRY}_lang={LANG}_model={MODEL}_eval={EVAL_METHOD}.csv\"\n",
    "\n",
    "            # if qidx == 77 and COUNTRY == \"egypt\":\n",
    "                #     print(f\"> Skipping Q77\")\n",
    "                #     continue\n",
    "\n",
    "            dump_dir = os.path.dirname(save_dump_path)\n",
    "            os.makedirs(dump_dir, exist_ok=True)\n",
    "\n",
    "            if qidx not in wvs_question_map:\n",
    "              print(f\" Skipping Q{values[0]}\")\n",
    "              continue\n",
    "\n",
    "            if not SCALE_QS and qidx in wvs_scale_questions:\n",
    "              print(f\" Skipping Q{values[0]} (Scale Q)\")\n",
    "              continue\n",
    "\n",
    "            if SCALE_QS and qidx not in wvs_scale_questions:\n",
    "              print(f\" Skipping Q{values[0]} (Not Scale Q)\")\n",
    "              continue\n",
    "\n",
    "            if str(qidx) not in wvs_response_map or f\"Q{qidx}\" not in wvs_questions:\n",
    "              print(f\" Skipping Q{values[0]}\")\n",
    "              continue\n",
    "\n",
    "            # From dictionary getting answer choices from given question\n",
    "            question_options = list(map(str.lower, wvs_questions[f\"Q{qidx}\"][\"options\"]))\n",
    "            print(\" Options\",  question_options)\n",
    "\n",
    "            # Load model responses (persona, question, response)\n",
    "            model_data = read_json(filepath)\n",
    "            print(\" Load raw responses\", len(model_data))\n",
    "\n",
    "            if len(model_data) != 4800 and COUNTRY == \"egypt\" and not (\"AceGPT\" in MODEL or 'Llama-2' in MODEL):\n",
    "                  filepath = filepath.replace(\"_v3\", \"_v1\")\n",
    "                  filepath = filepath.replace(\"_maxt=16\", \"_maxt=8\")\n",
    "                  model_data = read_json(filepath)\n",
    "\n",
    "              # try:\n",
    "              #     if \"AceGPT\" in MODEL or 'Llama-2' in MODEL:\n",
    "              #         assert len(model_data) >= 1212\n",
    "              #     elif COUNTRY == \"egypt\":\n",
    "              #         assert len(model_data) == 4800\n",
    "              #     elif COUNTRY == \"us\":\n",
    "              #         assert len(model_data) == 1200\n",
    "              # except:\n",
    "              #     print(len(model_data))\n",
    "\n",
    "            model_df, invalid_count = convert_to_dataframe(\n",
    "                  model_data,\n",
    "                  question_options,\n",
    "                  demographic_map,\n",
    "                  eval_method=EVAL_METHOD,\n",
    "                  language=LANG\n",
    "              )\n",
    "\n",
    "\n",
    "            all_invalids += invalid_count\n",
    "            all_num_responses += len(model_data)\n",
    "\n",
    "            model_df.to_csv(save_dump_path)\n",
    "\n",
    "          print(f\"{MODEL} | {LANG}: {all_invalids}/{all_num_responses}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxK--VhiDcjW"
   },
   "source": [
    "### Compute alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGQ4Iil3DrPc",
    "outputId": "1db987a8-44cb-44b3-dfa7-396695e4acaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persona Country: ['us']\n",
      "Skip Same Answer: False\n",
      "> Skipping ../dumps_wvs_2/q=2_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=19_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=21_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=42_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=62_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=63_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=77_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=78_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=83_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=84_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=87_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=88_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=124_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=126_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=127_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=142_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=143_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=149_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=150_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=171_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=175_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=199_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=209_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=210_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=221_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=224_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=229_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=235_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=236_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=239_country=us_lang=en_model=AceGPT-13B-chat_eval=mv_sample.csv\n",
      "> No results found for config: ('us', 'en', 'AceGPT-13B-chat')\n",
      "> Skipping ../dumps_wvs_2/q=126_country=us_lang=en_model=Llama-2-13b-chat-hf_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=2_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=19_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=21_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=42_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=62_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=63_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=77_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=78_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=83_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=84_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=87_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=88_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=124_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=126_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=127_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=142_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=143_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=149_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=150_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=171_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=175_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=199_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=209_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=210_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=221_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=224_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=229_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=235_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=236_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=239_country=us_lang=en_model=gpt-3.5-turbo-1106_eval=mv_sample.csv\n",
      "> No results found for config: ('us', 'en', 'gpt-3.5-turbo-1106')\n",
      "> Skipping ../dumps_wvs_2/q=2_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=19_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=21_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=42_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=62_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=63_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=77_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=78_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=83_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=84_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=87_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=88_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=124_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=126_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=127_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=142_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=143_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=149_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=150_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=171_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=175_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=199_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=209_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=210_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=221_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=224_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=229_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=235_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=236_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> Skipping ../dumps_wvs_2/q=239_country=us_lang=en_model=mt0-xxl_eval=mv_sample.csv\n",
      "> No results found for config: ('us', 'en', 'mt0-xxl')\n",
      "Results: 5847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumps Intersection: 5847\n",
      "Survey DF: 303\n",
      "Survey DF: 303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 13/31 [00:17<00:22,  1.26s/it]/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 31/31 [00:38<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('us', 'en', 'AceGPT-13B-chat')\n",
      "MAE: 0.5907943067033977\n",
      "Accuracy: 0.3483126721763085\n",
      "Nael MAE Score: 0.12822483450070746\n",
      "Nael Acc Score: 0.10418896051094526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load /content/drive/MyDrive/cultural-trends/scripts/wvs_majority_vote.py\n",
    "#%%writefile /content/drive/MyDrive/cultural-trends/scripts/wvs_majority_vote.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import scipy\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from utils import read_json, write_json, read_file, read_yaml, parse_range, parse_response_wvs, convert_to_percentages\n",
    "\n",
    "from utils import kl_divergence, create_wvs_question_map\n",
    "\n",
    "demographic_ids = [\"N_REGION_WVS Region country specific\", \"Q260 Sex\", \"Q262 Age\", \"Q273 Marital status\", \"Q275R Highest educational level: Respondent (recoded into 3 groups)\", \"Q287 Social class (subjective)\"]\n",
    "demographic_txt = [\"region\", \"sex\", \"age\", \"marital_status\", \"education\", \"social_class\"]\n",
    "\n",
    "# demographic_ids = [\"Q260 Sex\", \"Q262 Age\", \"Q273 Marital status\", \"Q275R Highest educational level: Respondent (recoded into 3 groups)\", \"Q287 Social class (subjective)\"]\n",
    "# demographic_txt = [\"sex\", \"age\", \"marital_status\", \"education\", \"social_class\"]\n",
    "\n",
    "columns_by = ['persona.region', 'persona.sex', 'persona.age', 'persona.marital_status', 'persona.education', 'persona.social_class', 'question.id', 'question.variant']\n",
    "\n",
    "# columns_by = ['persona.sex', 'persona.age', 'persona.marital_status', 'persona.education', 'persona.social_class', 'question.id', 'question.variant']\n",
    "region_id = \"N_REGION_WVS Region country specific\"\n",
    "\n",
    "# not_scale_questions = [\"Q19\", \"Q21\", \"Q149\", \"Q150\", \"Q171\", \"Q175\", \"Q209\"*, \"Q210\"*, \"Q221\"*]\n",
    "not_scale_questions = [\"Q19\", \"Q21\", \"Q149\", \"Q150\", \"Q171\", \"Q175\", \"Q209\", \"Q210\", \"Q221\"]\n",
    "\n",
    "from functools import reduce\n",
    "from typing import Union\n",
    "\n",
    "def dataframe_intersection(\n",
    "    dataframes: list[pd.DataFrame], by: Union[list, str]\n",
    "):\n",
    "\n",
    "    # set_index = [d.set_index(by) for d in dataframes]\n",
    "    # index_intersection = reduce(pd.Index.intersection, [d.index for d in set_index])\n",
    "    # intersected = [df.loc[index_intersection].reset_index() for df in set_index]\n",
    "\n",
    "    visited_personas = []\n",
    "    for df in dataframes:\n",
    "        visited = set()\n",
    "        df.set_index(by, inplace=True)\n",
    "        for index_tuple in df.index:\n",
    "            visited.add(index_tuple)\n",
    "        visited_personas += [visited]\n",
    "\n",
    "    intersected = list(set.intersection(*visited_personas))\n",
    "\n",
    "    i_dataframes = []\n",
    "    for df in dataframes:\n",
    "        i_dataframes += [df.loc[intersected].reset_index()]\n",
    "\n",
    "    return i_dataframes, intersected\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    LANGS = [\"en\"] # ar\n",
    "    MODELS_COUNTRY = [\"us\"]\n",
    "    SURVEY_COUNTRY = \"us\"\n",
    "\n",
    "    # MODELS = ['AceGPT-13B-chat', 'Llama-2-13b-chat-hf', \"mt0-xxl\", \"gpt-3.5-turbo-0613\"]\n",
    "    MODELS = ['AceGPT-13B-chat', 'Llama-2-13b-chat-hf', \"gpt-3.5-turbo-1106\", \"mt0-xxl\"]\n",
    "\n",
    "    EVAL_METHOD = \"mv_sample\" # {\"flatten\", \"mv_sample\", \"mv_all\"}\n",
    "    SCALE_QS = False # {False, True}\n",
    "\n",
    "    SKIP_SAME_ANS =  False\n",
    "\n",
    "    selected_questions = read_file(\"../dataset/selected_questions.csv\")[0].split(\",\")\n",
    "    # selected_questions_2 = read_file(\"filtered_questions_by_mae.csv\")[0].split(\",\")\n",
    "\n",
    "    skip_questions = [234]\n",
    "\n",
    "    json_results = []\n",
    "    q_json_results = []\n",
    "    persona_json_results = []\n",
    "\n",
    "    selected_questions = list(map(str.strip, selected_questions))\n",
    "    selected_questions = [int(qnum[1:]) for qnum in selected_questions]\n",
    "    wvs_themes = parse_range(read_json(\"../dataset/wvs_themes.json\"))\n",
    "    options_dict = parse_range(read_json(\"../dataset/wvs_options.json\"))\n",
    "\n",
    "    # invalid_questions = [19, 42, 62, 63, 78, 83, 84, 87, 88, 126, 142, 149, 150, 171, 224, 229, 234, 235, 239]\n",
    "\n",
    "    print(f\"Persona Country: {MODELS_COUNTRY}\")\n",
    "    print(f\"Skip Same Answer: {SKIP_SAME_ANS}\")\n",
    "    all_results = []\n",
    "    if SURVEY_COUNTRY == \"egypt\":\n",
    "         #survey_path = \"../dataset/F00013297-WVS_Wave_7_Egypt_CsvTxt_v5.0.csv\"\n",
    "        survey_path = \"../dataset/eg_wvs_wave7_v7_n303.csv\"\n",
    "        other_survey_path = \"../dataset/F00013339-WVS_Wave_7_United_States_CsvTxt_v5.0.csv\"\n",
    "    elif SURVEY_COUNTRY == \"us\":\n",
    "        #survey_path = \"../dataset/F00013339-WVS_Wave_7_United_States_CsvTxt_v5.0.csv\"\n",
    "        survey_path = \"../dataset/us_wvs_wave7_v7_n303.csv\"\n",
    "        #other_survey_path = \"../dataset/F00013339-WVS_Wave_7_United_States_CsvTxt_v5.0.csv\"\n",
    "        other_survey_path = \"../dataset/us_wvs_wave7_v7_n303.csv\"\n",
    "\n",
    "    survey_df = pd.read_csv(survey_path, header=0, delimiter=\";\")\n",
    "    other_survey_df = pd.read_csv(other_survey_path, header=0, delimiter=\";\")\n",
    "    if SURVEY_COUNTRY == \"egypt\":\n",
    "        survey_df[region_id] = survey_df[region_id].apply(lambda x: x.replace(\"EG: \", \"\"))\n",
    "        other_survey_df[region_id] = other_survey_df[region_id].apply(lambda x: x.split(\":\")[-1][3:].strip())\n",
    "    else:\n",
    "        survey_df[region_id] = survey_df[region_id].apply(lambda x: x.split(\":\")[-1][3:].strip())\n",
    "        other_survey_df[region_id] = other_survey_df[region_id].apply(lambda x: x.replace(\"EG: \", \"\"))\n",
    "\n",
    "    wvs_question_map = create_wvs_question_map(survey_df.columns.tolist(), selected_questions)\n",
    "    other_wvs_question_map = create_wvs_question_map(other_survey_df.columns.tolist(), selected_questions)\n",
    "    wvs_response_map = read_json(\"../dataset/wvs_response_map.json\")\n",
    "    str_columns = ['persona.region', 'persona.sex', 'persona.country', 'persona.marital_status', 'persona.education', 'persona.social_class']\n",
    "    # columns = ['persona.region', 'persona.sex', 'persona.age', 'persona.country', 'persona.marital_status', 'persona.education', 'persona.social_class', 'question.id', 'question.variant', 'response.id']\n",
    "    # wvs_response_map_reverse = {}\n",
    "    # for qid, q_response_data in wvs_response_map.items():\n",
    "    #     wvs_response_map_reverse[qid] = {val: key for key, val in q_response_data.items()}\n",
    "\n",
    "    result_config = []\n",
    "    for model_country in MODELS_COUNTRY:\n",
    "        for MODEL in MODELS:\n",
    "            for LANG in LANGS:\n",
    "                question_results = []\n",
    "                result_config += [(model_country, LANG, MODEL)]\n",
    "                for qidx in selected_questions:\n",
    "                    if qidx in skip_questions: continue\n",
    "                    # if qidx in invalid_questions: continue\n",
    "                    # q=236_country=us_lang=en_model=gpt-3.5-turbo-0613_eval=mv_sample.csv\n",
    "\n",
    "                    results_path = f\"../dumps_wvs_2/q={qidx}_country={model_country}_lang={LANG}_model={MODEL}_eval={EVAL_METHOD}.csv\"\n",
    "\n",
    "                    if os.path.exists(results_path):\n",
    "                        results_df = pd.read_csv(results_path).drop(columns='Unnamed: 0')\n",
    "                        for col in str_columns:\n",
    "                            results_df[col] = results_df[col].str.lower()\n",
    "\n",
    "                        results_df[\"model\"] = [MODEL]*len(results_df)\n",
    "                        results_df[\"language\"] = [LANG]*len(results_df)\n",
    "                        results_df[\"theme\"] = [wvs_themes[qidx]]*len(results_df)\n",
    "                        results_df[\"model-country\"] = [model_country]*len(results_df)\n",
    "                        results_df[\"survey-country\"] = [SURVEY_COUNTRY]*len(results_df)\n",
    "                        question_results += [results_df]\n",
    "                    else:\n",
    "                        # breakpoint()\n",
    "                        print(f\"> Skipping {results_path}\")\n",
    "                if question_results: # Add this check\n",
    "                    all_results += [pd.concat(question_results, ignore_index=True)]\n",
    "                else:\n",
    "                    print(f\"> No results found for config: {result_config[-1]}\") # Optional: Add a message\n",
    "\n",
    "    # columns_by = ['persona.region', 'persona.sex', 'persona.age', 'persona.marital_status', 'persona.country', 'persona.education', 'persona.social_class', 'question.id', 'question.variant']\n",
    "    # columns_by = ['persona.region', 'persona.sex', 'persona.age']\n",
    "    print(f\"Results: {len(all_results[0])}\")\n",
    "\n",
    "    for result in all_results:\n",
    "        result.sort_values(by=columns_by, inplace=True)\n",
    "\n",
    "    results, personas = dataframe_intersection(all_results, columns_by)\n",
    "\n",
    "    result_json = []\n",
    "    unique_personas = set()\n",
    "    for persona_tuple in personas:\n",
    "        unique_personas.add(persona_tuple[:-2])\n",
    "\n",
    "    for result in tqdm(results):\n",
    "        remove_indices = []\n",
    "        visited_model_persona = set()\n",
    "        for row_idx, row in result.iterrows():\n",
    "            persona_tuple = tuple([str(row[col]).lower() for col in columns_by])\n",
    "            if persona_tuple not in visited_model_persona:\n",
    "                visited_model_persona.add(persona_tuple)\n",
    "            else:\n",
    "                remove_indices += [row_idx]\n",
    "\n",
    "        for remove_idx in remove_indices[::-1]:\n",
    "            result.drop(remove_idx, inplace=True)\n",
    "\n",
    "    print(f\"Dumps Intersection: {len(results[0])}\")\n",
    "\n",
    "    survey_filtered_df = []\n",
    "    remove_indices = []\n",
    "    visited_personas = set()\n",
    "    for survey_row_idx, survey_row in survey_df.iterrows():\n",
    "        survey_persona_tuple = tuple([str(survey_row[col]).lower() for col in demographic_ids])\n",
    "        if survey_persona_tuple in visited_personas:\n",
    "            remove_indices += [survey_row_idx]\n",
    "        else:\n",
    "            visited_personas.add(survey_persona_tuple)\n",
    "\n",
    "    print(f\"Survey DF: {len(survey_df)}\")\n",
    "    for remove_idx in remove_indices[::-1]:\n",
    "        survey_df.drop(remove_idx, inplace=True)\n",
    "\n",
    "    print(f\"Survey DF: {len(survey_df)}\")\n",
    "    # breakpoint()\n",
    "    for qidx in selected_questions:\n",
    "        if qidx in skip_questions: continue\n",
    "\n",
    "        response_map = {key: int(val) for key, val in wvs_response_map[str(qidx)].items()}\n",
    "        response_map |= {key: val+1 for val, key in enumerate(options_dict[qidx])}\n",
    "        response_map[\"No answer\"] = -1\n",
    "\n",
    "        survey_df[wvs_question_map[qidx]] = survey_df[wvs_question_map[qidx]].apply(lambda x: response_map[x])\n",
    "        other_survey_df[other_wvs_question_map[qidx]] = other_survey_df[other_wvs_question_map[qidx]].apply(lambda x: response_map[x])\n",
    "\n",
    "    survey_percentages_final = {}\n",
    "    for result_idx, result in enumerate(results):\n",
    "        persona_results = []\n",
    "        persona_exact, persona_random = [], []\n",
    "        random_accuracy = []\n",
    "        mae_score = []\n",
    "        for qidx in tqdm(selected_questions):\n",
    "            if qidx in skip_questions: continue\n",
    "\n",
    "            # response_map = {key: int(val) for key, val in wvs_response_map[str(qidx)].items()}\n",
    "            # response_map |= {key: val+1 for val, key in enumerate(options_dict[qidx])}\n",
    "            # response_map[\"No answer\"] = -1\n",
    "\n",
    "            # survey_df[wvs_question_map[qidx]] = survey_df[wvs_question_map[qqidx]].apply(lambda x: response_map[x])\n",
    "            question_result_df = result[result[\"question.id\"] == f\"Q{qidx}\"]\n",
    "            for d_id in demographic_ids:\n",
    "                if d_id == \"Q262 Age\": continue\n",
    "                survey_df[d_id] = survey_df[d_id].apply(str.lower)\n",
    "                other_survey_df[d_id] = other_survey_df[d_id].apply(str.lower)\n",
    "\n",
    "            question_mae_score = []\n",
    "            question_acc_score = []\n",
    "\n",
    "\n",
    "            for persona in unique_personas:\n",
    "                persona_row = question_result_df[(question_result_df[\"persona.region\"] == persona[0]) & (question_result_df[\"persona.sex\"] == persona[1]) & (question_result_df[\"persona.age\"] == persona[2]) & (question_result_df[\"persona.marital_status\"] == persona[3]) &\n",
    "                    (question_result_df[\"persona.education\"] == persona[4]) &\n",
    "                    (question_result_df[\"persona.social_class\"] == persona[5])\n",
    "                ]\n",
    "\n",
    "                survey_persona_row = survey_df[\n",
    "                    (survey_df[demographic_ids[0]] == persona[0]) &\n",
    "                    (survey_df[demographic_ids[1]] == persona[1]) &\n",
    "                    (survey_df[demographic_ids[2]] == persona[2]) &\n",
    "                    (survey_df[demographic_ids[3]] == persona[3]) &\n",
    "                    (survey_df[demographic_ids[4]] == persona[4]) &\n",
    "                    (survey_df[demographic_ids[5]] == persona[5])\n",
    "                ]\n",
    "\n",
    "                other_survey_persona_row = other_survey_df[\n",
    "                    (other_survey_df[demographic_ids[1]] == persona[1]) &\n",
    "                    (other_survey_df[demographic_ids[2]] == persona[2]) &\n",
    "                    (other_survey_df[demographic_ids[3]] == persona[3]) &\n",
    "                    (other_survey_df[demographic_ids[4]] == persona[4]) &\n",
    "                    (other_survey_df[demographic_ids[5]] == persona[5])\n",
    "                ]\n",
    "\n",
    "                try:\n",
    "                    survey_answer = survey_persona_row[wvs_question_map[qidx]].item()\n",
    "                    other_survey_answer = other_survey_persona_row[other_wvs_question_map[qidx]].item()\n",
    "                    model_answers = persona_row[\"response.answer\"].tolist()\n",
    "\n",
    "                    # rand_answers = persona_row[\"\"]\n",
    "\n",
    "                    if survey_answer == other_survey_answer and SKIP_SAME_ANS:\n",
    "                        continue\n",
    "\n",
    "                    # max_option = np.max(list(wvs_response_map_reverse[str(qidx)].keys()))\n",
    "                    if survey_answer == -1 or options_dict[qidx][survey_answer-1] in [\"No answer\"]:\n",
    "                        continue\n",
    "\n",
    "                    num_options = len(options_dict[qidx])\n",
    "                    assert 1 <= survey_answer <= num_options\n",
    "\n",
    "                    # if \"Don't know\" in options_dict[qidx]:\n",
    "                    #     num_options -= 1\n",
    "\n",
    "                    # if f\"Q{qidx}\" in not_scale_questions:\n",
    "                    #     # persona_exact += [x!=y]\n",
    "                    #     continue\n",
    "\n",
    "                    # num_options = np.sum([option_val not in [\"Don't know\", \"No answer\"] and option_idx > 0 for option_idx, option_val in wvs_response_map_reverse[str(qidx)].items()])\n",
    "                    for variant_idx, model_answer in enumerate(model_answers):\n",
    "                        # x = model_answer\n",
    "                        # y = survey_answer\n",
    "\n",
    "\n",
    "                        assert 1 <= survey_answer <= num_options\n",
    "                        assert 1 <= model_answer <= num_options\n",
    "\n",
    "                        # if options_dict[qidx][model_answer-1] == \"Don't know\":\n",
    "                        #     continue\n",
    "\n",
    "\n",
    "                        # else:\n",
    "                        #     exact = 1 - abs(x - y) / (num_options-1)\n",
    "                        #     exact = exact * (options_dict[qidx][model_answer-1] != \"Don't know\")\n",
    "\n",
    "                        #     random = sum([1 - abs(i + 1 - y) / (num_options-1) for i in range(num_options)]) / num_options\n",
    "\n",
    "                        #     persona_exact += [exact]\n",
    "                        #     persona_random += [random]\n",
    "\n",
    "                        if model_answer != -1:\n",
    "\n",
    "                            random_accuracy += [1/num_options]\n",
    "                            question_acc_score += [model_answer==survey_answer]\n",
    "\n",
    "                            if  options_dict[qidx][model_answer-1] == \"Don't know\" or \\\n",
    "                                options_dict[qidx][survey_answer-1] == \"Don't know\" or \\\n",
    "                                f\"Q{qidx}\" in not_scale_questions:\n",
    "\n",
    "                                question_mae_score += [model_answer==survey_answer]\n",
    "                                persona_random += [1/num_options]\n",
    "\n",
    "                            else:\n",
    "                                num_options_q = num_options - 1 if \"Don't know\" in options_dict[qidx] else num_options\n",
    "                                assert 1 <= model_answer <= num_options_q\n",
    "                                mae = abs(model_answer - survey_answer) / (num_options_q-1)\n",
    "                                assert 0 <= mae <= 1\n",
    "                                question_mae_score += [1 - mae]\n",
    "\n",
    "                                persona_random += [sum([1 - abs(i + 1 - survey_answer) / (num_options_q-1) for i in range(num_options_q)]) / num_options_q]\n",
    "\n",
    "\n",
    "                        persona_json_results += [{\n",
    "                            \"question\": qidx,\n",
    "                            \"variant\": variant_idx,\n",
    "                            **{d_id: persona[d_idx] for d_idx, d_id in enumerate(columns_by[:-2])},\n",
    "                            \"model-country\": result_config[result_idx][0],\n",
    "                            \"survey-country\": SURVEY_COUNTRY,\n",
    "                            \"prompting-language\": result_config[result_idx][1],\n",
    "                            \"model\": result_config[result_idx][2],\n",
    "                            \"mae-score\": question_mae_score[-1],\n",
    "                            \"accuracy\": question_acc_score[-1],\n",
    "                            \"model-answer\": model_answer,\n",
    "                            \"survey-answer\": survey_answer,\n",
    "                            \"rand-accuracy\": random_accuracy[-1],\n",
    "                            \"person-random\": persona_random[-1],\n",
    "\n",
    "                        }]\n",
    "\n",
    "                except:\n",
    "                    breakpoint()\n",
    "\n",
    "\n",
    "\n",
    "            mae_score.extend(question_mae_score)\n",
    "            persona_results.extend(question_acc_score)\n",
    "\n",
    "            q_json_results += [{\n",
    "                \"question\": qidx,\n",
    "                \"config\": result_config[result_idx],\n",
    "                \"model-country\": result_config[result_idx][0],\n",
    "                \"survey-country\": SURVEY_COUNTRY,\n",
    "                \"prompting-language\": result_config[result_idx][1],\n",
    "                \"model\": result_config[result_idx][2],\n",
    "                \"mae-score\": np.mean(question_mae_score),\n",
    "                \"accuracy\": np.mean(question_acc_score),\n",
    "            }]\n",
    "\n",
    "        print(f\"{result_config[result_idx]}\")\n",
    "\n",
    "        mae_score_final = np.mean(mae_score)\n",
    "        final_random = np.mean(persona_random)\n",
    "        score = (mae_score_final - final_random) / (1 - final_random)\n",
    "\n",
    "        acc_final = np.mean(persona_results)\n",
    "        acc_random_final = np.mean(random_accuracy)\n",
    "        nael_acc_score = (acc_final - acc_random_final) / (1 - acc_random_final)\n",
    "\n",
    "        print(f\"MAE: {mae_score_final}\")\n",
    "        print(f\"Accuracy: {acc_final}\")\n",
    "        print(f\"Nael MAE Score: {score}\")\n",
    "        print(f\"Nael Acc Score: {nael_acc_score}\")\n",
    "        print()\n",
    "\n",
    "        json_results += [{\n",
    "            \"config\": result_config[result_idx],\n",
    "            \"model-country\": result_config[result_idx][0],\n",
    "            \"survey-country\": SURVEY_COUNTRY,\n",
    "            \"prompting-language\": result_config[result_idx][1],\n",
    "            \"model\": result_config[result_idx][2],\n",
    "            \"mae-score\": mae_score_final,\n",
    "            \"accuracy\": acc_final,\n",
    "            \"nael-mae-score\": score,\n",
    "            \"nael-acc-score\": nael_acc_score\n",
    "        }]\n",
    "\n",
    "    #write_json(f\"results_{SURVEY_COUNTRY}.json\", json_results)\n",
    "    #write_json(f\"q_results_{SURVEY_COUNTRY}.json\", q_json_results)\n",
    "\n",
    "    if SKIP_SAME_ANS:\n",
    "        write_json(f\"persona_results_{SURVEY_COUNTRY}_{MODELS_COUNTRY}_filtered.json\", persona_json_results)\n",
    "    else:\n",
    "        write_json(f\"persona_results_{SURVEY_COUNTRY}_filtered_all_3.json\", persona_json_results)\n",
    "\n",
    "        # print(f\"{result_config[result_idx]}: {score} | {np.mean(final_exact)} | {len(persona_exact)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCc45yOGEuL4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"persona_results_us_filtered_all_3.json\")\n",
    "df.to_csv(\"persona_results_us_filtered_all_3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "p3CXAADdN6fv",
    "9HE4r4R0DnT6",
    "sYwxWvaVCFmY",
    "-IaNR9TzXQSf",
    "u4SKXachXYsz",
    "-vQep8T9DSXA"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
